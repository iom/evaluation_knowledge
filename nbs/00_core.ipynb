{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook implements an evidence mapping system with:\n",
    "\n",
    " - Batch processing for scalability\n",
    "\n",
    " - Robust error handling and retries\n",
    "\n",
    " - Embedding caching\n",
    "\n",
    " - Hybrid search (vector + full-text)\n",
    " \n",
    " - Local LanceDB deployment\n",
    "\n",
    "we can follow these steps:\n",
    "\n",
    " - Load the JSON file containing the URLs of the PDF reports.\n",
    " - Load the Excel file describing the IOM Results Framework.\n",
    " - Download and process the PDF reports to extract text.\n",
    " - Integrate the extracted text with the IOM Results Framework.\n",
    " - Generate embeddings and store them in LanceDB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Create a Virtual Environment\n",
    "\n",
    "To ensure a clean and isolated environment for this project, we will create a virtual environment using Python's `venv` module. This will help manage dependencies and avoid conflicts with other projects.\n",
    "\n",
    "```{bash} \n",
    "#| eval: false\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "Then, activate the virtual environment:\n",
    "```{bash} \n",
    "#| eval: false\n",
    ".\\.venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "\n",
    "Then, configure visual Studio Code to use the virtual environment: Open the Command Palette using the shortcut `Ctrl+Shift+P` and type `Jupyter: Select Interpreter` and select the interpreter that corresponds to your newly created virtual environment: `('venv': venv)`.\n",
    "\n",
    "\n",
    "### Required Python Modules\n",
    "\n",
    "Once this environment selected as a kernel to run the notebook, we can install the required python modules the rest of the process:\n",
    "\n",
    "```{python} \n",
    "%pip install openai  lancedb pyarrow pandas numpy matplotlib seaborn plotly pymupdf requests tqdm tenacity ipython dotenv langchain langchain-community langchain_openai  ipywidgets openpyxl  filetype\n",
    "```\n",
    "\n",
    "\n",
    "then Restart the jupyter kernel for this notebook\n",
    "```{python}\n",
    "#| eval: false\n",
    "%reset -f\n",
    "```\n",
    "\n",
    "### Initialise LLM API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|label: llmapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI \n",
    "# Initialize LLM with higher temperature for creative question generation\n",
    "llm_creative = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "llm_accurate = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load PDF library and Strategic Results Framework\n",
    "\n",
    "The library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: load_evaluations\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import json\n",
    "def load_evaluations(file_path,json_path):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(file_path, sheet_name='extract from 2005 to Aug 2024', engine='openpyxl')\n",
    "    \n",
    "    # Filter evaluations from 2005 to August 2024\n",
    "    filtered_df = df \n",
    "    \n",
    "    # Create a nested structure\n",
    "    evaluations = []\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        evaluation = {\n",
    "            \"Title\": row[\"Title\"],\n",
    "            \"Year\": str(row[\"Year\"]),\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Best Practices or Lessons Learnt\": row[\"Best Practicesor Lessons Learnt\"],\n",
    "            \"Date of Publication\": str(row[\"Date of Publication\"]),\n",
    "            \"Donor\": row[\"Donor\"],\n",
    "            \"Evaluation Brief\": row[\"Evaluation Brief\"],\n",
    "            \"Evaluation Commissioner\": row[\"Evaluation Commissioner\"],\n",
    "            \"Evaluation Coverage\": row[\"Evaluation Coverage\"],\n",
    "            \"Evaluation Period From Date\": str(row[\"Evaluation Period From Date\"]),\n",
    "            \"Evaluation Period To Date\": str(row[\"Evaluation Period To Date\"]),\n",
    "            \"Executive Summary\": row[\"Executive Summary\"],\n",
    "            \"External Version of the Report\": row[\"External Version of the Report\"],\n",
    "            \"Languages\": row[\"Languages\"],\n",
    "            \"Migration Thematic Areas\": row[\"Migration Thematic Areas\"],\n",
    "            \"Name of Project(s) Being Evaluated\": row[\"Name of Project(s) Being Evaluated\"],\n",
    "            \"Number of Pages Excluding annexes\": row[\"Number of Pages Excluding annexes\"],\n",
    "            \"Other Documents Included\": row[\"Other Documents Included\"],\n",
    "            \"Project Code\": row[\"Project Code\"],\n",
    "            \"Countries Covered\": [country.strip() for country in str(row[\"Countries Covered\"]).split(\",\")],\n",
    "            \"Regions Covered\": row[\"Regions Covered\"],\n",
    "            \"Relevant Crosscutting Themes\": row[\"Relevant Crosscutting Themes\"],\n",
    "            \"Report Published\": row[\"Report Published\"],\n",
    "            \"Terms of Reference\": row[\"Terms of Reference\"],\n",
    "            \"Type of Evaluation Scope\": row[\"Type of Evaluation Scope\"],\n",
    "            \"Type of Evaluation Timing\": row[\"Type of Evaluation Timing\"],\n",
    "            \"Type of Evaluator\": row[\"Type of Evaluator\"],\n",
    "            \"Level of Evaluation\": row[\"Level of Evaluation\"],\n",
    "            \"Documents\": []\n",
    "        }\n",
    "        \n",
    "        # Split the document-related fields by comma and create a list of dictionaries\n",
    "        document_subtypes = str(row[\"Document Subtype\"]).split(\", \")\n",
    "        file_urls = str(row[\"File URL\"]).split(\", \")\n",
    "        file_descriptions = str(row[\"File description\"]).split(\", \")\n",
    "        \n",
    "        for subtype, url, description in zip(document_subtypes, file_urls, file_descriptions):\n",
    "            document = {\n",
    "                \"Document Subtype\": subtype,\n",
    "                \"File URL\": url,\n",
    "                \"File description\": description\n",
    "            }\n",
    "            evaluation[\"Documents\"].append(document)\n",
    "        \n",
    "        evaluations.append(evaluation)\n",
    "\n",
    "    ## dump data\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(evaluations, json_file, indent=4)\n",
    "\n",
    "    \n",
    "    return evaluations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "library =load_evaluations(\"reference/Evaluation_repository.xlsx\",\"reference/Evaluation_repository.json\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: load_iom_framework\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "def load_iom_framework(excel_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and validate IOM framework\"\"\"\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Validate expected columns\n",
    "    required_columns = ['Objective', 'Outcome', 'Indicator']\n",
    "    for col in required_columns:\n",
    "        assert col in df.columns, f\"Framework missing required column: {col}\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "framework= load_iom_framework(\"reference/Strategic_Result_Framework.xlsx\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 1: Building the Knowledge Base\n",
    "\n",
    "So we have a collection of Evaluation documents. We have metadata for each Evaluation. For each evaluation, we have multiple documents (The evaluation report itslef, plus in some case: a summary brief, annexes, etc.)\n",
    "\n",
    "See an example below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "[\n",
    "    {\n",
    "        \"Title\": \"Finale Internal Evluation: ENHANCING THE CAPACITY TO MAINSTREAM ENVIRONMENT AND CLIMATE CHANGE WITHIN WIDER FRAMEWORK OF MIGRATION MANAGEMENT IN WEST AND CENTRAL AFRICA\",\n",
    "        \"Year\": \"2022\",\n",
    "        \"Author\": \"Abderrahim El Moulat\",\n",
    "        \"Best Practices or Lessons Learnt\": \"Yes\",\n",
    "        \"Date of Publication\": \"2022-06-22 00:00:00\",\n",
    "        \"Donor\": \"IOM Development Fund\",\n",
    "        \"Evaluation Brief\": \"Yes\",\n",
    "        \"Evaluation Commissioner\": \"Donor, IOM\",\n",
    "        \"Evaluation Coverage\": \"Country\",\n",
    "        \"Evaluation Period From Date\": \"nan\",\n",
    "        \"Evaluation Period To Date\": \"NaT\",\n",
    "        \"Executive Summary\": \"Yes\",\n",
    "        \"External Version of the Report\": \"No\",\n",
    "        \"Languages\": \"English\",\n",
    "        \"Migration Thematic Areas\": \"Migration and climate change\",\n",
    "        \"Name of Project(s) Being Evaluated\": NaN,\n",
    "        \"Number of Pages Excluding annexes\": 20.0,\n",
    "        \"Other Documents Included\": NaN,\n",
    "        \"Project Code\": \"NC.0030\",\n",
    "        \"Countries Covered\": [\n",
    "            \"Senegal\"\n",
    "        ],\n",
    "        \"Regions Covered\": \"RO Dakar\",\n",
    "        \"Relevant Crosscutting Themes\": \"Gender\",\n",
    "        \"Report Published\": \"Yes\",\n",
    "        \"Terms of Reference\": \"No\",\n",
    "        \"Type of Evaluation Scope\": \"Programme/Project\",\n",
    "        \"Type of Evaluation Timing\": \"Ex-post (after the end of the project/programme)\",\n",
    "        \"Type of Evaluator\": \"Internal\",\n",
    "        \"Level of Evaluation\": \"Decentralized\",\n",
    "        \"Documents\": [\n",
    "            {\n",
    "                \"Document Subtype\": \"Evaluation brief\",\n",
    "                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Internal%20Evaluation_NC0030_JUNE_2022_FINAL_Abderrahim%20EL%20MOULAT_0.pdf\",\n",
    "                \"File description\": \"Evaluation Brief\"\n",
    "            },\n",
    "            {\n",
    "                \"Document Subtype\": \"Evaluation report\",\n",
    "                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/NC0030_Evaluation%20Brief_June%202022_Abderrahim%20EL%20MOULAT.pdf\",\n",
    "                \"File description\": \"Evaluation Report\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"Title\": \"Local Authorities Network for Migration and Development\",\n",
    "        \"Year\": \"2022\",\n",
    "        \"Author\": \"Action Research for CO-development (ARCO)\",\n",
    "        \"Best Practices or Lessons Learnt\": \"No\",\n",
    "        \"Date of Publication\": \"2022-02-01 00:00:00\",\n",
    "        \"Donor\": \"Government of Italy\",\n",
    "        \"Evaluation Brief\": \"No\",\n",
    "        \"Evaluation Commissioner\": \"IOM\",\n",
    "        \"Evaluation Coverage\": \"Multi-country\",\n",
    "        \"Evaluation Period From Date\": \"2020-07-06 00:00:00\",\n",
    "        \"Evaluation Period To Date\": \"2021-07-31 00:00:00\",\n",
    "        \"Executive Summary\": \"Yes\",\n",
    "        \"External Version of the Report\": \"No\",\n",
    "        \"Languages\": \"English\",\n",
    "        \"Migration Thematic Areas\": \"Migration and Development - diaspora\",\n",
    "        \"Name of Project(s) Being Evaluated\": NaN,\n",
    "        \"Number of Pages Excluding annexes\": 37.0,\n",
    "        \"Other Documents Included\": NaN,\n",
    "        \"Project Code\": \"MD.0003\",\n",
    "        \"Countries Covered\": [\n",
    "            \"Albania\",\n",
    "            \"Italy\"\n",
    "        ],\n",
    "        \"Regions Covered\": \"RO Brussels\",\n",
    "        \"Relevant Crosscutting Themes\": \"Gender, Rights-based approach\",\n",
    "        \"Report Published\": \"No\",\n",
    "        \"Terms of Reference\": \"Yes\",\n",
    "        \"Type of Evaluation Scope\": \"Programme/Project\",\n",
    "        \"Type of Evaluation Timing\": \"Final (at the end of the project/programme)\",\n",
    "        \"Type of Evaluator\": \"External\",\n",
    "        \"Level of Evaluation\": \"Decentralized\",\n",
    "        \"Documents\": [\n",
    "            {\n",
    "                \"Document Subtype\": \"Evaluation report\",\n",
    "                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Evaluation%20Brief_ARCO_Shiraz%20JERBI.pdF\",\n",
    "                \"File description\": \"Evaluation Report \"\n",
    "            },\n",
    "            {\n",
    "                \"Document Subtype\": \"Evaluation brief\",\n",
    "                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Final%20evaluation%20report_ARCO_Shiraz%20JERBI_1.pdf\",\n",
    "                \"File description\": \"Evaluation Brief\"\n",
    "            },\n",
    "            {\n",
    "                \"Document Subtype\": \"Management response\",\n",
    "                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Management%20Response%20Matrix_ARCO_Shiraz%20JERBI.pdf\",\n",
    "                \"File description\": \"Management Response\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Workflow Overview\n",
    "\n",
    "1. Load external metadata for one evaluations as per json file.\n",
    "2. Then for each evaluation, download the file, convert it to PDF (in case it's a word, excel or ppt), and load text for each document.\n",
    "3. Then implement a [late chunking that can solve the lost context problem](https://isaacflath.com/blog/blog_post?fpath=posts%2F2025-04-08-LateChunking.ipynb) and  insert in chunk table, enabling [Hybrid Search capability](https://docs.lancedb.com/core/hybrid-search) so that search can be made based on both key words and similirarity.\n",
    "4. Create additional metadata for both documents and for evaluation using an LLM call. the additional metadata shall help to define an asesssment of the \"evidence strenght\". The metadata to be created are - evaluation type (formative, summative, impact), Methodology, study design, sample size, and data collection techniques  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Initialize LanceDB Vector Database\n",
    "\n",
    "The database includes 23 tables: \n",
    "\n",
    "__1. Evaluations Table__\n",
    "Each row represents a unique evaluation with the following fields:\n",
    "\n",
    "* evaluation_id (unique identifier)\n",
    "* title \n",
    "* author\n",
    "* practice_or_lessons\n",
    "* donor\n",
    "* is_brief\n",
    "* commissioner\n",
    "* coverage\n",
    "* countries\n",
    "* from_date\n",
    "* to_date\n",
    "* has_summary\n",
    "* external_version\n",
    "* language\n",
    "* thematic_area\n",
    "* name_project\n",
    "* project_code\n",
    "* evaluation_scope\n",
    "* evaluation_timing\n",
    "* evaluation_level\n",
    "* evaluator_type\n",
    "* theme\n",
    "* cross_cutting\n",
    "\n",
    "additional variable will be generated through an LLM prompt on the entire evaluation content\n",
    "\n",
    "* short_title \n",
    "* summary\n",
    "* population (PICO model)\n",
    "* intervention (PICO model)\n",
    "* comparator (PICO model)\n",
    "* outcome (PICO model)\n",
    "* methodology\n",
    "* study_type\n",
    "* study_design\n",
    "* sample_size\n",
    "* data_collection_techniques\n",
    "* evidence_strength \n",
    "* limitations \n",
    "\n",
    "\n",
    "__2. Documents Table__\n",
    "\n",
    "Each row represents a PDF file linked to an evaluation:\n",
    " \n",
    "* document_id: Primary key   ID of the original PDF\n",
    "* evaluation_id: foreign key to link to the evaluation\n",
    "* document_subtype: from the original metadata\n",
    "* document_url: from the original metadata\n",
    "* document_name: from the original metadata \n",
    "* document_tite:  document type as reviewed by the LLM\n",
    "* document_type_infer: document type as reviewed by the LLM\n",
    "* document_processed: boolean to confirm it is done\n",
    "\n",
    "\n",
    "__3. Chunk Table__\n",
    "* chunk_id: Primary key  \n",
    "* evaluation_id: foreign key to link to the evaluation\n",
    "* document_id: ID of the original file\n",
    "* document_page: for proper referencing of any further information retrieval\n",
    "* chunk_index: order of the chunk in the document\n",
    "* text: the chunked content\n",
    "* embedding (for hybrid search)\n",
    "\n",
    "Let's start by loading the library from json...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: load_evaluations\n",
    "\n",
    "from typing import List, Dict, Optional # Type hinting\n",
    "import json\n",
    "def load_evaluations(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load evaluation data from a JSON file\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing evaluation data\n",
    "        \n",
    "    Returns:\n",
    "        List of evaluation dictionaries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Handle both single evaluation and list of evaluations\n",
    "        if isinstance(data, dict):\n",
    "            return [data]\n",
    "        elif isinstance(data, list):\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Invalid JSON structure - expected object or array\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file {json_path}: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation data: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a small subset for testing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Load your   metadata\n",
    "#evaluation_data =  load_evaluations(\"reference/Evaluation_repository_test.json\")\n",
    "evaluation_data =  load_evaluations(\"reference/Evaluation_repository.json\")\n",
    "print(f\"Attribute name is: {evaluation_data}\")\n",
    "print(type(evaluation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_id\n",
    "\n",
    "import hashlib\n",
    "def generate_id(text: str) -> str:\n",
    "    \"\"\"Generate a deterministic ID from text\"\"\"\n",
    "    return hashlib.md5(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "eval_id = generate_id( \"aaa\")\n",
    "print({eval_id})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: force_delete_directory\n",
    "\n",
    "import time\n",
    "import shutil\n",
    "def force_delete_directory(path, max_retries=3, delay=1):\n",
    "    \"\"\"Robust directory deletion with retries and delay\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to delete {path} after {max_retries} attempts: {e}\")\n",
    "                return False\n",
    "            time.sleep(delay)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "force_delete_directory(LANCE_DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We start prefilling our vector database with the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: initialise_knowledge_base\n",
    "\n",
    "from lancedb import connect\n",
    "import numpy as np\n",
    "\n",
    "def safe_get(d, key, default=None):\n",
    "    \"\"\"Safely get value from dict, handle NaN and missing keys\"\"\"\n",
    "    value = d.get(key, default)\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return default\n",
    "    return value\n",
    "\n",
    "def initialise_knowledge_base(db, evaluation: Dict):\n",
    "    \"\"\"Store full documents without chunking (late chunking approach)\"\"\"\n",
    "\n",
    "    import pyarrow as pa\n",
    "    from typing import Optional\n",
    "\n",
    "    # EvaluationModel Schema\n",
    "    evaluation_schema = pa.schema([\n",
    "        pa.field(\"evaluation_id\", pa.string()),\n",
    "        pa.field(\"title\", pa.string()),\n",
    "        pa.field(\"year\", pa.string()),\n",
    "        pa.field(\"author\", pa.string()),\n",
    "        pa.field(\"donor\", pa.string()),\n",
    "        pa.field(\"evaluation_commissioner\", pa.string()),\n",
    "        pa.field(\"migration_thematic_areas\", pa.string()),\n",
    "        pa.field(\"relevant_crosscutting_themes\", pa.string()),\n",
    "        pa.field(\"type_of_evaluation_timing\", pa.string()),\n",
    "        pa.field(\"type_of_evaluator\", pa.string()),\n",
    "        pa.field(\"level_of_evaluation\", pa.string()),\n",
    "        pa.field(\"scope\", pa.string()),\n",
    "        pa.field(\"geography\", pa.list_(pa.string())),\n",
    "        pa.field(\"summary\", pa.string()),\n",
    "        pa.field(\"evaluation_type\", pa.string()),\n",
    "        pa.field(\"population\", pa.string()),\n",
    "        pa.field(\"intervention\", pa.string()),\n",
    "        pa.field(\"comparator\", pa.string()),\n",
    "        pa.field(\"outcome\", pa.string()),\n",
    "        pa.field(\"methodology\", pa.string()),\n",
    "        pa.field(\"study_design\", pa.string()),\n",
    "        pa.field(\"sample_size\", pa.string()),\n",
    "        pa.field(\"data_collection_techniques\", pa.string()),\n",
    "        pa.field(\"evidence_strength\", pa.string()),\n",
    "        pa.field(\"limitations\", pa.string())\n",
    "    ])\n",
    "\n",
    "    # DocumentModel Schema\n",
    "    document_schema = pa.schema([\n",
    "        pa.field(\"document_id\", pa.string()),\n",
    "        pa.field(\"url\", pa.string()),\n",
    "        pa.field(\"description\", pa.string()),\n",
    "        pa.field(\"evaluation_id\", pa.string()),\n",
    "        pa.field(\"content\", pa.string()),\n",
    "        pa.field(\"processed\", pa.bool_()),\n",
    "        pa.field(\"document_title\", pa.string()),\n",
    "        pa.field(\"document_type_infer\", pa.string())\n",
    "    ])\n",
    "\n",
    "    # Create or open tables with explicit schema\n",
    "    try:\n",
    "        eval_table = db.create_table(\"evaluations\", schema=evaluation_schema, exist_ok=True)\n",
    "        doc_table = db.create_table(\"documents\", schema=document_schema, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        raise\n",
    "\n",
    "    eval_id = generate_id(f\"{evaluation['Title']}_{evaluation['Year']}\")\n",
    "    # Prepare the evaluation record\n",
    "    eval_record = {\n",
    "        \"evaluation_id\": eval_id,\n",
    "        \"title\": evaluation[\"Title\"],\n",
    "        \"year\": evaluation[\"Year\"],\n",
    "        \"author\": safe_get(evaluation, \"Author\"),\n",
    "        \"donor\": safe_get(evaluation, \"Donor\"),\n",
    "        \"evaluation_commissioner\": safe_get(evaluation, \"Evaluation Commissioner\"),\n",
    "        \"migration_thematic_areas\": safe_get(evaluation, \"Migration Thematic Areas\"),\n",
    "        \"relevant_crosscutting_themes\": safe_get(evaluation, \"Relevant Crosscutting Themes\"),\n",
    "        \"type_of_evaluation_timing\": safe_get(evaluation, \"Type of Evaluation Timing\"),\n",
    "        \"type_of_evaluator\": safe_get(evaluation, \"Type of Evaluator\"),\n",
    "        \"level_of_evaluation\": safe_get(evaluation, \"Level of Evaluation\"),\n",
    "        \"scope\": safe_get(evaluation, \"Type of Evaluation Scope\"),\n",
    "        \"geography\": safe_get(evaluation, \"Countries Covered\", []),\n",
    "\n",
    "        ## rest will be filled later by the generate_evaluation_metadata()\n",
    "        \"summary\": None,\n",
    "        \"evaluation_type\": None,\n",
    "        \"population\": None,\n",
    "        \"intervention\": None,\n",
    "        \"comparator\": None,\n",
    "        \"outcome\": None,\n",
    "        \"methodology\": None,\n",
    "        \"study_design\": None,\n",
    "        \"sample_size\": None,\n",
    "        \"data_collection_techniques\": None,\n",
    "        \"evidence_strength\": None,\n",
    "        \"limitations\": None\n",
    "    }\n",
    " \n",
    "    # Insert evaluation (convert to RecordBatch first)\n",
    "    eval_batch = pa.RecordBatch.from_pylist([eval_record], schema=evaluation_schema)  \n",
    "    eval_table.add(eval_batch)  \n",
    "    #print(f\"Eval Record to insert: {eval_batch}\")\n",
    "\n",
    "    documents = []\n",
    "    for doc in evaluation.get(\"Documents\", []):\n",
    "        doc_id = generate_id(doc[\"File URL\"])\n",
    "        documents.append({\n",
    "            \"document_id\": doc_id,\n",
    "            \"url\": doc[\"File URL\"],            \n",
    "            \"description\": doc[\"File description\"],\n",
    "            \"evaluation_id\": eval_id,\n",
    "            \"content\": \"\",  # Will be filled when processed,\n",
    "            \"processed\": False,            \n",
    "            \"document_title\": None,\n",
    "            \"document_type_infer\": None \n",
    "        })\n",
    "    \n",
    "   # print(f\"Document to insert: {documents}\")\n",
    "    # Insert documents with schema validation\n",
    "    if documents:\n",
    "        try:\n",
    "            #if \"documents\" in db.table_names():\n",
    "            #    db.drop_table(\"documents\")\n",
    "\n",
    "            # doc_table = db.create_table(\"documents\", schema=document_schema)\n",
    "            \n",
    "            schema_fields = [f.name for f in document_schema]\n",
    "            documents_ordered = [{k: doc[k] for k in schema_fields} for doc in documents]\n",
    "            # Create Arrow table ensuring schema match\n",
    "            doc_data = pa.Table.from_pylist(documents_ordered)            \n",
    "            # Align with expected schema\n",
    "            aligned_data = doc_data.cast(document_schema)            \n",
    "            # Add to LanceDB table\n",
    "            doc_table.add(aligned_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            print(f\"Document schema: \\n  {doc_data.schema} \\n \\n\")\n",
    "            print(f\"Expected schema: {document_schema}\")\n",
    "            raise\n",
    "\n",
    "    return eval_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "LANCE_DB_PATH = \"./lancedb\"\n",
    "db = connect(LANCE_DB_PATH)\n",
    "for evaluation in evaluation_data:  # Assuming evaluation_data is a list\n",
    "    initialise_knowledge_base(db, evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's check each evaluation is in the DB -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "eval_table = db.open_table(\"evaluations\")\n",
    "#  Convert to Pandas DataFrame (recommended for display)\n",
    "df = eval_table.to_pandas()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "and the corresponding documents...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "LANCE_DB_PATH = \"./lancedb\"\n",
    "from lancedb import connect\n",
    "db = connect(LANCE_DB_PATH)\n",
    "doc_table = db.open_table(\"documents\")\n",
    "#  Convert to Pandas DataFrame (recommended for display)\n",
    "df = doc_table.to_pandas()\n",
    "print(df)\n",
    "# this table includes document_id, url, and evaluation_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Download and prepare all the files\n",
    "\n",
    "Now we build a smart function to download the files from  URL:\n",
    " - this function takes an argument the `doc_table` from the vector DB (`doc_table = db.open_table(\"documents\")`). this table includes document_id, url, and evaluation_id\n",
    " - then for each document, and in parallelised way, it loads the url and extract the `file_name` from the `url` within the table\n",
    " - it builds a local `file_path` with `PDF_Library`/`evaluation_id`/`file_name` (where `PDF_Library` is an environment variable) \n",
    " - it checks if the 'file_name' is already present and then gracefully exit\n",
    " - if not, it downloads the file_name - this done with with some provision to avoid requesting IP being banned - and ensure some retry until the file is downloaded\n",
    " - if the file_name extension is not pdf, it identify the file extension then it converts it to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: download_documents\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import filetype  # For detecting file type\n",
    " \n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 5  # base seconds\n",
    "THREADS = 5\n",
    "THROTTLE_DELAY_RANGE = (1, 3)  # Delay between downloads per thread\n",
    "\n",
    "# Sample pool of common User-Agents\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\",\n",
    "]\n",
    "\n",
    "\n",
    "def download_documents(doc_table):\n",
    "    pdf_root = Path(os.getenv(\"PDF_Library\", \"./PDF_Library\"))\n",
    "\n",
    "    def process_doc(doc):\n",
    "        document_id = doc['document_id']\n",
    "        url = doc['url']\n",
    "        evaluation_id = doc['evaluation_id']\n",
    "\n",
    "        try:\n",
    "            file_name = os.path.basename(urlparse(url).path)\n",
    "            print(f\"processing {file_name}\")\n",
    "            if not file_name:\n",
    "                file_name = f\"{document_id}.pdf\"  # fallback\n",
    "\n",
    "            file_dir = pdf_root / str(evaluation_id)\n",
    "            file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = file_dir / file_name\n",
    "\n",
    "            if file_path.exists():\n",
    "                return f\"[✓] Skipped {file_name} (already exists)\"\n",
    "\n",
    "            # Rate throttling (add jitter)\n",
    "            time.sleep(random.uniform(*THROTTLE_DELAY_RANGE))\n",
    "\n",
    "            # Retry logic for downloading\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "                    response = requests.get(url, headers=headers, timeout=15)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                        break\n",
    "                    else:\n",
    "                        raise Exception(f\"Status {response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    if attempt == MAX_RETRIES:\n",
    "                        return f\"[✗] Failed to download {file_name} after {MAX_RETRIES} attempts: {e}\"\n",
    "                    time.sleep(RETRY_DELAY * attempt + random.uniform(0.5, 2.0))  # exponential backoff + jitter\n",
    "\n",
    "\n",
    "            # Filetype validation\n",
    "            kind = filetype.guess(file_path)\n",
    "            actual_extension = file_path.suffix.lower().lstrip('.')\n",
    "\n",
    "            # Check if it's already a PDF by detected type or file extension (case-insensitive)\n",
    "            is_pdf = (kind and kind.extension.lower() == 'pdf') or actual_extension == 'pdf'\n",
    "\n",
    "            if not is_pdf:\n",
    "                pdf_path = file_path.with_suffix('.pdf')\n",
    "                print(f\"The file {file_path} is not a pdf and shall be converted\")\n",
    "                convert_file_to_pdf(file_path, pdf_path)\n",
    "                file_path.unlink()  # remove original\n",
    "                return f\"[→] Converted {file_name} to PDF\"\n",
    "            return f\"[✓] Downloaded {file_name}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"[✗] Error processing {url}: {e}\"\n",
    "\n",
    "\n",
    "    # Fetch documents from table\n",
    "    documents = doc_table.to_pandas().to_dict(orient=\"records\")\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        for result in tqdm(executor.map(process_doc, documents), total=len(documents)):\n",
    "            results.append(result)\n",
    "\n",
    "    for r in results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here is the file conversion functions that assumes that [libre-office](https://www.libreoffice.org/download/download-libreoffice/) is installed locally.\n",
    "\n",
    "```{bash}\n",
    "# Debian/Ubuntu\n",
    "sudo apt install libreoffice\n",
    "\n",
    "# Mac (Homebrew)\n",
    "brew install --cask libreoffice\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: convert_file_to_pdf\n",
    "\n",
    "import subprocess\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def find_libreoffice_exec():\n",
    "    \"\"\"\n",
    "    Finds the appropriate LibreOffice command based on OS.\n",
    "    Returns path to LibreOffice CLI tool or raises an error.\n",
    "    \"\"\"\n",
    "    system = platform.system()\n",
    "\n",
    "    # Windows typically installs LibreOffice here\n",
    "    if system == \"Windows\":\n",
    "        possible_paths = [\n",
    "            r\"C:\\Program Files\\LibreOffice\\program\\soffice.exe\",\n",
    "            r\"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe\"\n",
    "        ]\n",
    "        for path in possible_paths:\n",
    "            if Path(path).exists():\n",
    "                return path\n",
    "        raise FileNotFoundError(\"LibreOffice not found on Windows. Please install it or set it in PATH.\")\n",
    "    \n",
    "    # On macOS, typically installed via brew or dmg\n",
    "    elif system == \"Darwin\":\n",
    "        possible_paths = [\n",
    "            \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        ]\n",
    "        for path in possible_paths:\n",
    "            if Path(path).exists():\n",
    "                return path\n",
    "        # fallback to PATH\n",
    "        return shutil.which(\"soffice\") or shutil.which(\"libreoffice\")\n",
    "\n",
    "    # On Linux, assume it's installed via apt/yum/pacman\n",
    "    elif system == \"Linux\":\n",
    "        return shutil.which(\"libreoffice\") or shutil.which(\"soffice\")\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unsupported operating system: {system}\")\n",
    "\n",
    "def convert_file_to_pdf(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Converts Word, Excel, or PowerPoint file to PDF using LibreOffice in headless mode.\n",
    "    Works on Windows, macOS, and Linux.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file does not exist: {input_path}\")\n",
    "\n",
    "    try:\n",
    "        libreoffice_exec = find_libreoffice_exec()\n",
    "        if not libreoffice_exec or not Path(libreoffice_exec).exists():\n",
    "            raise FileNotFoundError(\"LibreOffice executable not found.\")\n",
    "\n",
    "        # LibreOffice generates PDF in the same folder as input, same base name\n",
    "        subprocess.run([\n",
    "            libreoffice_exec,\n",
    "            \"--headless\",\n",
    "            \"--convert-to\", \"pdf\",\n",
    "            \"--outdir\", str(output_path.parent),\n",
    "            str(input_path)\n",
    "        ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        generated_pdf = input_path.with_suffix('.pdf')\n",
    "        expected_pdf = output_path\n",
    "\n",
    "        if generated_pdf.exists():\n",
    "            generated_pdf.rename(expected_pdf)\n",
    "        elif expected_pdf.exists():\n",
    "            pass  # already saved there\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"PDF was not generated for: {input_path.name}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(f\"LibreOffice failed: {e.stderr.decode().strip()}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Conversion error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing this...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "doc_table = db.open_table(\"documents\")\n",
    "os.environ[\"PDF_Library\"] = \"Evaluation_Library\"\n",
    "download_documents(doc_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Now load file content in the vector DB, chunk and embedd\n",
    "\n",
    "Building a function that \n",
    " - this function takes an argument the `doc_table` from the vector DB (`doc_table = db.open_table(\"documents\")`). this table includes document_id, url, and evaluation_id, processed\n",
    " - then for each document, and in parallelised way, it loads the url and extract the `file_name` from the `url` within the table\n",
    " - it assume a local `file_path` with `PDF_Library`/`evaluation_id`/`file_name` (where `PDF_Library` is an environment variable - `file_name` is extracted from the url - and the `file_name` extension is sanitised to include systematically '.pdf' ) \n",
    " - It will extract the text from the PDF using PyMuPDF with error handling\n",
    "-- it will implement the  It will then fill in the chunk table in lancedb, implementing a late chunking approach to avoid duplicate embedding computation, ensure context-aware chunk boundaries and precise span tracking .\n",
    "- the lancedb chunk table schema should be \n",
    "    - chunk_id: str\n",
    "    - document_id: str\n",
    "    - evaluation_id: str\n",
    "    - metadata: dict[str, str] \n",
    "    - content: str = embedding_fn.SourceField()\n",
    "    - vector: Vector(embedding_fn.ndims()) = embedding_fn.VectorField() \n",
    "- Once processed the  processed variable in doc_table is set to true   \n",
    "\n",
    "\n",
    "Test the embeddings through lanchain....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: false\n",
    "#| echo: true\n",
    "#|label: initialise_embeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    deployment=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION_EMBED\"),\n",
    "    chunk_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "test_embedding = embedding_model.embed_query(\"Hello world\")\n",
    "print(f\"Embedding vector length: {len(test_embedding)}\")\n",
    "embedding_dim = len(test_embedding)\n",
    "# LanceDB-compatible wrapper\n",
    "class LangchainEmbeddingWrapper:\n",
    "    def __init__(self, langchain_embedder):\n",
    "        self._embedder = langchain_embedder\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        return self._embedder.embed_documents(texts)\n",
    "        \n",
    "    def ndims(self):\n",
    "        return self._dim\n",
    "\n",
    "# Wrap and use\n",
    "embedding_fn = LangchainEmbeddingWrapper(embedding_model)\n",
    "#print(\"Embedding dimension:\", embedding_fn.ndims())\n",
    "\n",
    "vec = embedding_fn([\"Hello world\"])\n",
    "print(f\"Vector through lancedb dim: {len(vec[0])}\")\n",
    "print(embedding_fn([\"Hello world\"])[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "print(dir(embedding_fn))\n",
    "help(embedding_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So first we create the chunk table in lancedb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from pydantic import BaseModel\n",
    "from lancedb.pydantic import Vector\n",
    "import pyarrow as pa \n",
    "pa_schema = pa.schema([\n",
    "    pa.field(\"chunk_id\", pa.string()),\n",
    "    pa.field(\"document_id\", pa.string()),\n",
    "    pa.field(\"evaluation_id\", pa.string()),\n",
    "    pa.field(\"metadata\", pa.string()),  # storing metadata dict as JSON string for simplicity\n",
    "    pa.field(\"content\", pa.string()),\n",
    "    pa.field(\"vector\", pa.list_(pa.float32(), embedding_dim)),  # vector as list of floats\n",
    "])\n",
    "\n",
    "from lancedb import connect\n",
    "db = connect(LANCE_DB_PATH)\n",
    "chunk_table = db.create_table(\"chunks\", schema=pa_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "and then the function creating embeddings chunck for each document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: process_documents_to_chunks\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import uuid\n",
    "import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from lancedb.pydantic import Vector\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF = 2  # seconds base (exponential)\n",
    "os.environ[\"PDF_Library\"] = \"Evaluation_Library\"\n",
    "LOG_FILE = \"chunck_processing.log\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def process_documents_to_chunks(doc_table, chunk_table):\n",
    "    PDF_LIBRARY = os.environ[\"PDF_Library\"]\n",
    "    \n",
    "    # --- Helper: Extract text from PDF ---\n",
    "    def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\\n\".join(page.get_text() for page in doc)\n",
    "            doc.close()\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # --- Helper: Sanitize file name ---\n",
    "    def sanitize_filename_from_url(url: str) -> str:\n",
    "        file_name = Path(url.split(\"?\")[0]).name\n",
    "        return Path(file_name).stem + \".pdf\"\n",
    "\n",
    "    # --- Helper: Split text into chunks ---\n",
    "    def chunk_text(text: str) -> List[str]:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
    "            length_function=len,\n",
    "            keep_separator=True,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "\n",
    "    embedding_dim = \"5000\"\n",
    "\n",
    "    # --- Helper: Embed with retry ---\n",
    "    def embed_with_retry(text: str) -> List[float]:\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                return embedding_fn([text])[0]\n",
    "            except Exception as e:\n",
    "                wait_time = RETRY_BACKOFF ** attempt\n",
    "                logging.warning(f\"Embedding failed (attempt {attempt}): {e}\")\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    raise\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "    # --- Threaded processing function ---\n",
    "    def process_doc(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        document_id = doc[\"document_id\"]\n",
    "        if doc.get(\"processed\", False):\n",
    "            logging.info(f\"Already processed: {document_id}\")\n",
    "            return {\"document_id\": document_id, \"status\": \"already_processed\", \"chunks\": 0}\n",
    "\n",
    "        url = doc[\"url\"]\n",
    "        evaluation_id = doc[\"evaluation_id\"]\n",
    "        file_name = sanitize_filename_from_url(url)\n",
    "        file_path = Path(PDF_LIBRARY) / evaluation_id / file_name\n",
    "\n",
    "        logging.info(f\"Processing: {document_id} ({file_path})\")\n",
    "\n",
    "        if not file_path.exists():\n",
    "            logging.warning(f\"Missing file: {file_path}\")\n",
    "            return {\"document_id\": document_id, \"status\": \"missing_file\", \"chunks\": 0}\n",
    "\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            logging.warning(f\"No text extracted from: {file_path}\")\n",
    "            return {\"document_id\": document_id, \"status\": \"no_text_extracted\", \"chunks\": 0}\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        chunk_records = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                chunk_id = f\"{document_id}_{i}_{uuid.uuid4().hex[:6]}\"\n",
    "                vector = embed_with_retry(chunk)\n",
    "                chunk_records.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"document_id\": document_id,\n",
    "                    \"evaluation_id\": evaluation_id,\n",
    "                    \"metadata\": json.dumps({\"chunk_index\": str(i)}),\n",
    "                    \"content\": chunk,\n",
    "                    \"vector\": vector,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to embed chunk {i} of {document_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if chunk_records:\n",
    "            chunk_table.add(pd.DataFrame(chunk_records))\n",
    "            return {\"document_id\": document_id, \"status\": \"processed\", \"chunks\": len(chunk_records)}\n",
    "        else:\n",
    "            logging.warning(f\"No chunks processed for {document_id}\")\n",
    "            return {\"document_id\": document_id, \"status\": \"no_chunks_processed\", \"chunks\": 0}\n",
    "\n",
    "    # Load documents\n",
    "    documents = doc_table.to_pandas().to_dict(orient=\"records\")\n",
    "\n",
    "    # Threaded processing\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        results = list(executor.map(process_doc, documents))\n",
    "\n",
    "    # Sequential update to avoid LanceDB conflicts\n",
    "    for result in results:\n",
    "        if result[\"status\"] == \"processed\":\n",
    "            document_id = result[\"document_id\"]\n",
    "            success = False\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    doc_table.update(\n",
    "                        where=f\"document_id = '{document_id}'\",\n",
    "                        values={\"processed\": True}\n",
    "                    )\n",
    "                    success = True\n",
    "                    break\n",
    "                except RuntimeError as e:\n",
    "                    if \"Retryable commit conflict\" in str(e):\n",
    "                        logging.warning(f\"Update conflict for {document_id}, retrying...\")\n",
    "                        time.sleep(RETRY_BACKOFF ** attempt)\n",
    "                    else:\n",
    "                        logging.error(f\"Failed to update {document_id}: {e}\")\n",
    "                        break\n",
    "            if not success:\n",
    "                logging.error(f\"Gave up updating {document_id} after retries\")\n",
    "\n",
    "    # Create full-text search index\n",
    "    chunk_table.create_fts_index(\"content\")\n",
    "    logging.info(\"FTS index created on 'content'\")\n",
    "\n",
    "    # Final report\n",
    "    for result in results:\n",
    "        logging.info(f\"{result['status']}: {result['document_id']} ({result['chunks']} chunks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let's run this! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "LANCE_DB_PATH = \"./lancedb\"\n",
    "from lancedb import connect\n",
    "db = connect(LANCE_DB_PATH)\n",
    "doc_table = db.open_table(\"documents\")\n",
    "chunk_table = db.open_table(\"chunks\")\n",
    "process_documents_to_chunks(doc_table, chunk_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Checking the status of the chunking process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: check_chunk_status\n",
    "\n",
    "def check_chunk_status(doc_table, chunk_table):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load documents and chunks as DataFrames\n",
    "    docs_df = doc_table.to_pandas()\n",
    "    chunks_df = chunk_table.to_pandas()\n",
    "\n",
    "    # Count chunks per document_id\n",
    "    chunk_counts = chunks_df.groupby(\"document_id\").size().reset_index(name=\"chunk_count\")\n",
    "\n",
    "    # Merge chunk counts into docs_df (left join)\n",
    "    merged = docs_df.merge(chunk_counts, on=\"document_id\", how=\"left\")\n",
    "\n",
    "    # Fill NaN chunk counts with 0 (documents with no chunks)\n",
    "    merged[\"chunk_count\"] = merged[\"chunk_count\"].fillna(0).astype(int)\n",
    "\n",
    "    # Define which docs are missing chunks or have zero chunks\n",
    "    missing_chunks_df = merged[merged[\"chunk_count\"] == 0]\n",
    "\n",
    "    # Summary counts\n",
    "    total_docs = len(docs_df)\n",
    "    properly_chunked = total_docs - len(missing_chunks_df)\n",
    "    missing_count = len(missing_chunks_df)\n",
    "\n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Properly chunked documents: {properly_chunked}\")\n",
    "    print(f\"Documents missing chunks: {missing_count}\")\n",
    "\n",
    "    if missing_count > 0:\n",
    "        print(\"\\nDocuments missing chunks:\")\n",
    "        for _, row in missing_chunks_df.iterrows():\n",
    "            doc_id = row[\"document_id\"]\n",
    "            url = row.get(\"url\", \"N/A\")\n",
    "            processed = row.get(\"processed\", False)\n",
    "            print(f\" - Document ID: {doc_id}, URL: {url}, Processed flag: {processed}\")\n",
    "\n",
    "    return missing_chunks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "LANCE_DB_PATH = \"./lancedb\"\n",
    "from lancedb import connect\n",
    "db = connect(LANCE_DB_PATH)\n",
    "doc_table = db.open_table(\"documents\")\n",
    "chunk_table = db.open_table(\"chunks\")\n",
    "missing_docs_df = check_chunk_status(doc_table, chunk_table)\n",
    "print(missing_docs_df)\n",
    "# this table includes document_id, url, and evaluation_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generating AI-Enhanced metadata \n",
    "\n",
    "Last, we run a function to generate metadata... \n",
    "\n",
    "The function will load the \"evaluations\" table within the db --connect(LANCE_DB_PATH) --\n",
    "then loop around each evaluation_id within the \"chunks\" table to retrive the context - and \n",
    "perform an LLM call to then generate as an output additional metadata\n",
    "Then it will update the evaluations table with the output for each evaluation -\n",
    "At the end it will save a json file with a dump of the evaluations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_evaluation_metadata\n",
    "\n",
    "# setting up for metadata generation\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from lancedb import connect\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI \n",
    "llm_accurate0 = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "## another instance with a better model that can handle longer context...\n",
    "llm_accurate = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"model_name\"),\n",
    "    api_key=os.getenv(\"subscription_key\"),\n",
    "    azure_endpoint=os.getenv(\"endpoint\"),\n",
    "    api_version=os.getenv(\"api_version\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=\"log/metadata_generation.log\", level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"Recursively clean an object to make it JSON serializable, handling None values.\"\"\"\n",
    "    if obj is None:\n",
    "        return []  # Convert None to empty list for join operations\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif hasattr(obj, \"tolist\"):  # for other array-like objects\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, (str, int, float, bool)):\n",
    "        return obj\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [clean_json(v) for v in obj]    \n",
    "    elif isinstance(obj, (np.ndarray, np.generic)):\n",
    "        return obj.tolist()    \n",
    "    else:\n",
    "        return str(obj)  # fallback for any other type\n",
    "\n",
    "# Use this safer approach when building context in the prompt\n",
    "def safe_join(items, sep=', '):\n",
    "    if items is None:\n",
    "        return ''\n",
    "    try:\n",
    "        if hasattr(items, 'tolist'):  # Handle numpy arrays\n",
    "            items = items.tolist()\n",
    "        return sep.join(str(item) for item in items)\n",
    "    except (TypeError, AttributeError):\n",
    "        return '' \n",
    "\n",
    "      \n",
    "\n",
    "def call_llm_with_retries(prompt, max_retries=4, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm_accurate.invoke([HumanMessage(content=prompt)])\n",
    "            # Get the content directly from the AIMessage object\n",
    "            raw_output = response.content.strip()\n",
    "\n",
    "            # Try to parse the JSON\n",
    "            try:\n",
    "                parsed = json.loads(raw_output)\n",
    "                # Validate the structure isn't just repeating the same value\n",
    "                if isinstance(parsed, dict) and any(isinstance(v, list) and len(v) > 50 for v in parsed.values()):\n",
    "                    raise JSONDecodeError(\"Output contains excessively repeated values\", \"\", 0)\n",
    "                return parsed\n",
    "            except JSONDecodeError as e:\n",
    "                # Try to fix common issues\n",
    "                if raw_output.count('{') != raw_output.count('}'):\n",
    "                    # Try to complete the JSON if it was cut off\n",
    "                    if raw_output.count('{') > raw_output.count('}'):\n",
    "                        raw_output += '}'\n",
    "                    else:\n",
    "                        raw_output = '{' + raw_output\n",
    "                parsed = json.loads(raw_output)\n",
    "                return parsed\n",
    "                \n",
    "        except JSONDecodeError as e:\n",
    "            logging.warning(f\"JSON parsing failed (attempt {attempt + 1}). Output:\\n{raw_output}\")\n",
    "            if attempt == max_retries - 1:  # Last attempt\n",
    "                # Try to salvage partial data\n",
    "                try:\n",
    "                    # Extract the valid part before the cutoff\n",
    "                    valid_part = raw_output[:raw_output.rfind('}')+1]\n",
    "                    return json.loads(valid_part)\n",
    "                except:\n",
    "                    raise RuntimeError(f\"LLM returned malformed JSON that couldn't be repaired: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"LLM error (attempt {attempt + 1}): {str(e)}\")\n",
    "        time.sleep(delay * (2 ** attempt))\n",
    "    raise RuntimeError(\"LLM call failed after retries.\")\n",
    "\n",
    "\n",
    "from lancedb.rerankers import RRFReranker\n",
    "def get_context_for_eval(eval_row, query, chunk_table):\n",
    "    \n",
    "    reranker = RRFReranker()\n",
    "    evaluation_id = eval_row[\"evaluation_id\"]\n",
    "    query_embedding = embedding_fn(query)\n",
    "    results = chunk_table.search(query_embedding).where(\n",
    "        f\"evaluation_id = '{evaluation_id}'\", prefilter=True\n",
    "    ).limit(3).to_pandas()\n",
    "\n",
    "    if results.empty:\n",
    "        logging.warning(f\"No chunks found for evaluation_id={evaluation_id}\")\n",
    "        return None\n",
    "\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document:\\n{row['content']}\"\n",
    "        for _, row in results.iterrows()\n",
    "    ])\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the  __PICO structured framework__ as an approach to represent the causal knowledge found in the Evaluation (cf. [EconBERTa: Towards Robust Extraction of Named Entities in Economics](https://aclanthology.org/2023.findings-emnlp.774.pdf)). This scheme helps in systematically organizing and analyzing the effectiveness of interventions by comparing outcomes between groups:\n",
    "\n",
    "__1. Population (P)__:  The group of individuals or units (e.g., households, schools, firms) affected by the intervention. The target population shall be clearly defined (e.g., smallholder farmers, primary school students, unemployed youth) and it shall Include eligibility criteria (e.g., age, socioeconomic status, geographic location).\n",
    "\n",
    "__2. Intervention (I)__: The program, policy, or treatment whose effect is being evaluated. Describes the active component being tested (e.g., cash transfers, training workshops, new teaching methods). Should specify dosage, duration, and delivery mechanism.\n",
    "\n",
    "__3. Comparators (C)__: The counterfactual scenario—what would have happened without the intervention. Ideally involves a control group (if the study approach is randomized or quasi-experimental) that does not receive the intervention. Alternatively refers to \"Business-as-usual\" groups, placebo interventions, or different treatment arms.\n",
    "\n",
    "__4. Outcome (O)__:The measurable effects or endpoints used to assess the intervention’s impact. Includes primary outcomes (main indicators of interest, e.g., school enrollment rates, income levels) and secondary outcomes (e.g., health, empowerment). Should be specific, measurable, and time-bound (e.g., \"child literacy scores after 12 months\").\n",
    "\n",
    "\n",
    "Using such approach, we can ensure Clarity (the research question is well-defined and testable), Causal Inference (isolate the effect of the intervention by comparing treated and untreated groups), Replicability (to potentially extrapolate the findings) and Relevance (linking outcomes to real-world decision-making).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_metadata_for_evaluation_metadata_descriptive\n",
    "\n",
    "# descriptive\n",
    "from langchain.schema import HumanMessage\n",
    "from json import JSONDecodeError\n",
    "# Define query and embedding\n",
    "query_descriptive = \" Population, Intervention, Outcome, comparator\"\n",
    "\n",
    "def generate_metadata_for_evaluation_metadata_descriptive(eval_row, query_descriptive, chunk_table):\n",
    "    \"\"\"Process one evaluation row and return updated row with metadata.\"\"\"\n",
    "    evaluation_id = eval_row[\"evaluation_id\"]\n",
    "\n",
    "    context = get_context_for_eval(eval_row, query_descriptive, chunk_table)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    You are an expert in public program and policy evaluation implementation. \n",
    "    \n",
    "    Your task is to generate evaluation metadata related to a specific evaluation excercise:\n",
    "    \n",
    "    - Summary:  Provide a summary of the 5 key main generic recommandations from the evaluation\n",
    "\n",
    "    - Population:  The group of individuals or units (e.g., households, schools, firms) affected by the intervention. \n",
    "\n",
    "    - Intervention: The program, policy, or treatment whose effect is being evaluated. Describes the active component being tested (e.g., cash transfers, training workshops, new teaching methods). Should specify dosage, duration, and delivery mechanism.\n",
    "\n",
    "    - Comparators: The counterfactual scenario, i.e. what would have happened without the intervention. Ideally involves a control group (if the study approach is randomized or quasi-experimental) that does not receive the intervention. Alternatively refers to \"Business-as-usual\" groups, placebo interventions, or different treatment arms.\n",
    "\n",
    "    - Outcome:The measurable effects or endpoints used to assess the intervention’s impact. Includes primary outcomes (main indicators of interest, e.g., school enrollment rates, income levels) and secondary outcomes (e.g., health, empowerment). Should be specific, measurable, and time-bound (e.g., \"child literacy scores after 12 months\").\n",
    "\n",
    "\n",
    "    Below are some existing Metadata on this evaluation:\n",
    "    - Title: {eval_row.get('title')}\n",
    "    - Year: {eval_row.get('year')}\n",
    "    - Author: {eval_row.get('author')}\n",
    "    - Evaluation Coverage: {eval_row.get('level_of_evaluation')}\n",
    "    - Type of Evaluation Scope: {eval_row.get('scope')}\n",
    "    - Type of Evaluation Timing: {eval_row.get('type_of_evaluation_timing')}\n",
    "    - Thematic Areas: {eval_row.get('migration_thematic_areas')}\n",
    "    - Cross cutting themes: {eval_row.get('relevant_crosscutting_themes')}\n",
    "    - Countries: {safe_join(eval_row.get('geography'))}\n",
    "\n",
    "    and some Relevant Document Context:\n",
    "    {context}\n",
    "\n",
    "    Return ONLY valid JSON with the following structure:\n",
    "    {{\n",
    "        \"summary\":  \"\" ,\n",
    "        \"population\": [\"population1\", \"population2\", ...],\n",
    "        \"intervention\": [\"intervention1\", \"intervention2\", ...],\n",
    "        \"comparator\": [\"comparator1\", \"comparator2\", ...],\n",
    "        \"outcome\": [\"outcome1\", \"outcome2\", ...]\n",
    "    }}\n",
    "\n",
    "    Important Rules:\n",
    "    1. Each array should contain no more than 10 items\n",
    "    2. Items should be distinct and non-repetitive\n",
    "    3. Return ONLY the JSON object, no additional text\n",
    "    4. Anchor the response into the provided context and exisiting metadata\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "   # logging.info(f\"Sending prompt to LLM for evaluation_id={evaluation_id}:\\n{prompt}\")\n",
    "    \n",
    "    logging.info(f\"Sending prompt to LLM for Description evaluation_id={evaluation_id} \")\n",
    "    try:\n",
    "        metadata = call_llm_with_retries(prompt)\n",
    "        eval_row.update(metadata)\n",
    "        #logging.info(f\"Processed evaluation_id={evaluation_id}\")\n",
    "        logging.info(f\"Description {eval_row}\")\n",
    "        return eval_row\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM failed for Description of evaluation_id={evaluation_id} | {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_metadata_for_evaluation_metadata_methodo\n",
    "\n",
    "#methodo\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Define query and embedding\n",
    "query_methodo = \"Study design, Methodology, Sample, Data Collection\"\n",
    "\n",
    "def generate_metadata_for_evaluation_metadata_methodo(eval_row, query_methodo, chunk_table):\n",
    "    \"\"\"Process one evaluation row and return updated row with metadata.\"\"\"\n",
    "    evaluation_id = eval_row[\"evaluation_id\"]\n",
    "\n",
    "    context = get_context_for_eval(eval_row, query_methodo, chunk_table)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    You are an expert in public program and policy evaluation implementation. \n",
    "    \n",
    "    Your task is to generate the following evaluation metadata about the methodology from a specific evaluation excercise: \n",
    "    - evaluation type\n",
    "    - study_design\n",
    "    - methodology (qualitative, quantitative, mixed methods)\n",
    "    - sample_size: description of the sample used for the study\n",
    "    - data_collection_techniques (e.g. surveys, interviews, focus groups, document review).\n",
    " \n",
    "    Use the following list of evaluation type:\n",
    "    - Formative Evaluation: Conducted during development/implementation to improve the programme, \n",
    "    - Process Evaluation: Examine implementation fidelity and operations, \n",
    "    - Outcome Evaluation: Measures short-term/intermediate results (between output and impact), \n",
    "    - Summative Evaluation: Assess overall effectiveness after completion, \n",
    "    - Impact Evaluation: Measure long-term effects and causal attribution, \n",
    "    \n",
    "    Use the following list of study design types\n",
    "\n",
    "    - \"Observational - Cross-sectional: Data collected at a single point in time, no follow-up.\",\n",
    "    - \"Observational - Cohort: Participants followed over time without experimental manipulation.\",\n",
    "    - \"Experimental - Randomized Controlled Trial: Participants or units are randomly assigned to groups.\",\n",
    "    - \"Experimental - Quasi-experimental: Includes comparison or time series design without randomization.\",\n",
    "    - \"Hybrid Type 1: Primarily tests intervention effectiveness while collecting limited implementation data.\",\n",
    "    - \"Hybrid Type 2: Simultaneously tests intervention  and implementation strategies.\",\n",
    "    - \"Hybrid Type 3: Primarily tests implementation strategies while collecting limited intervention effectiveness data.\",\n",
    "    - \"Case Study / Mixed-methods: In-depth exploration of implementation in one or few settings using qualitative and/or quantitative data.\" \n",
    "\n",
    "    Below are some existing Metadata on this specific evaluation:\n",
    "    - Title: {eval_row.get('title')}\n",
    "    - Year: {eval_row.get('year')}\n",
    "    - Author: {eval_row.get('author')}\n",
    "    - Evaluation Coverage: {eval_row.get('level_of_evaluation')}\n",
    "    - Type of Evaluation Scope: {eval_row.get('scope')}\n",
    "    - Type of Evaluation Timing: {eval_row.get('type_of_evaluation_timing')}\n",
    "    - Thematic Areas: {eval_row.get('migration_thematic_areas')}\n",
    "    - Cross cutting themes: {eval_row.get('relevant_crosscutting_themes')}\n",
    "    - Countries: {safe_join(eval_row.get('geography'))}\n",
    "    - Summary: {eval_row.get('summary')}\n",
    "    - Populations: {safe_join(eval_row.get('population'))}\n",
    "    - Interventions: {safe_join(eval_row.get('intervention' ))}\n",
    "    - Compators: {safe_join(eval_row.get('comparator'))}\n",
    "    - Outcomes: {safe_join(eval_row.get('outcome'))}\n",
    "\n",
    "\n",
    "    and some Relevant Document Context  on this specific evaluation:\n",
    "    {context}\n",
    "\n",
    "    Return ONLY valid JSON with the following structure:\n",
    "    {{\n",
    "        \"methodology\":  \"\" ,\n",
    "        \"evaluation_type\":  [\"evaluation_type1\", \"evaluation_type2\", ...] ,\n",
    "        \"study_design\":  \"\",\n",
    "        \"sample_size\":  \"\" , \n",
    "        \"data_collection_techniques\": [\"data_collection_techniques1\", \"data_collection_techniques2\", ...] \n",
    "    }}\n",
    "\n",
    "    Important Rules:\n",
    "    1. Each array should contain no more than 10 items\n",
    "    2. Items should be distinct and non-repetitive\n",
    "    3. Return ONLY the JSON object, no additional text\n",
    "    4. Anchor the response into the provided context and exisiting metadata\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "   # logging.info(f\"Sending prompt to LLM for evaluation_id={evaluation_id}:\\n{prompt}\")\n",
    "    \n",
    "    logging.info(f\"Sending prompt to LLM for methodology  evaluation_id={evaluation_id} \")\n",
    "    try:\n",
    "        metadata = call_llm_with_retries(prompt)\n",
    "        eval_row.update(metadata)\n",
    "        #logging.info(f\"Processed methodology  evaluation_id={evaluation_id}\")\n",
    "        logging.info(f\" {eval_row}\")\n",
    "        return eval_row\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM failed for methodology of evaluation_id={evaluation_id} | {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_metadata_for_evaluation_metadata_evidence\n",
    "\n",
    "# evidence\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Define query and embedding\n",
    "query = \"Evidence Limitation findings recommendations\"\n",
    "\n",
    "def generate_metadata_for_evaluation_metadata_evidence(eval_row, query, chunk_table):\n",
    "    \"\"\"Process one evaluation row and return updated row with metadata.\"\"\"\n",
    "    evaluation_id = eval_row[\"evaluation_id\"]\n",
    "\n",
    "    context = get_context_for_eval(eval_row, query, chunk_table)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    You are an expert in public program and policy evaluation implementation. \n",
    "    \n",
    "    Your task is to generate evaluation metadata about the evaluation output:\n",
    "     - evidence_strength \n",
    "     - limitations \n",
    " \n",
    "\n",
    "    Below are some existing Metadata on this evaluation:\n",
    "    - Title: {eval_row.get('title')}\n",
    "    - Year: {eval_row.get('year')}\n",
    "    - Author: {eval_row.get('author')}\n",
    "    - Evaluation Coverage: {eval_row.get('level_of_evaluation')}\n",
    "    - Type of Evaluation Scope: {eval_row.get('scope')}\n",
    "    - Type of Evaluation Timing: {eval_row.get('type_of_evaluation_timing')}\n",
    "    - Thematic Areas: {eval_row.get('migration_thematic_areas')}\n",
    "    - Cross cutting themes: {eval_row.get('relevant_crosscutting_themes')}\n",
    "    - Countries: {safe_join(eval_row.get('geography', []))}\n",
    "    - Summary: {eval_row.get('summary')}\n",
    "    - Populations: {safe_join(eval_row.get('population'))}\n",
    "    - Interventions: {safe_join(eval_row.get('intervention'))}\n",
    "    - Compators: {safe_join(eval_row.get('comparator'))}\n",
    "    - Outcomes: {safe_join(eval_row.get('outcome'))}\n",
    "    - Study Design: {eval_row.get('study_design')}\n",
    "    - Evaluation type:  {safe_join(eval_row.get('evaluation_type'))}\n",
    "    - Methodology: {eval_row.get('methodology')}\n",
    "    - Sample size: {eval_row.get('sample_size')}\n",
    "    - Data collection techniques:  {safe_join(eval_row.get('data_collection_techniques'))}\n",
    "\n",
    " \n",
    "    and some Relevant Document Context:\n",
    "    {context}\n",
    "\n",
    "    Return ONLY valid JSON with the following structure:\n",
    "    {{\n",
    "        \"evidence_strength\":  \"\" ,\n",
    "        \"limitations\":  [\"limitations1\", \"limitations2\", ...]  \n",
    "    }}\n",
    "\n",
    "    Important Rules:\n",
    "    1. Each array should contain no more than 10 items\n",
    "    2. Items should be distinct and non-repetitive\n",
    "    3. Return ONLY the JSON object, no additional text\n",
    "    4. Anchor the response into the provided context and exisiting metadata\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "   # logging.info(f\"Sending prompt to LLM for evaluation_id={evaluation_id}:\\n{prompt}\")\n",
    "    \n",
    "    logging.info(f\"Sending prompt to LLM for Evidence evaluation_id={evaluation_id} \")\n",
    "    try:\n",
    "        metadata = call_llm_with_retries(prompt)\n",
    "        eval_row.update(metadata)\n",
    "        #logging.info(f\"Processed Evidence evaluation_id={evaluation_id}\")\n",
    "        logging.info(f\" {eval_row}\")\n",
    "        return eval_row\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM failed for Evidence evaluation_id={evaluation_id} | {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|eval: true\n",
    "#| echo: true\n",
    "#|label: generate_evaluation_metadata\n",
    "\n",
    "# process all\n",
    "def generate_evaluation_metadata(\n",
    "    eval_table,\n",
    "    chunk_table, \n",
    "    batch_size=10, \n",
    "    output_file=\"all_evaluations_metadata.json\"):\n",
    "    \"\"\"Main function to generate metadata in batches and update the table, with incremental saving.\"\"\"\n",
    "    all_rows = eval_table.to_pandas().to_dict(orient=\"records\")\n",
    "    enriched_data = []\n",
    "    total = len(all_rows)\n",
    "    \n",
    "    logging.info(f\"processing {total} evaluations!\")\n",
    "    success = 0\n",
    "    skipped = 0\n",
    "\n",
    "    # Initialize or load existing data if file exists\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            enriched_data = json.load(f)\n",
    "            logging.info(f\"Loaded existing data with {len(enriched_data)} entries\")\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        pass\n",
    "\n",
    "    for i in range(0, len(all_rows), batch_size):\n",
    "        batch = all_rows[i:i+batch_size]\n",
    "        enriched_batch = []\n",
    "             \n",
    "        for row in batch:\n",
    "            enriched = generate_metadata_for_evaluation_metadata_descriptive(row, query, chunk_table)\n",
    "            if enriched:\n",
    "                row = enriched  # update row with enriched version\n",
    "                success += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "            enriched = generate_metadata_for_evaluation_metadata_methodo(row, query, chunk_table)\n",
    "            if enriched:\n",
    "                row = enriched\n",
    "                success += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "            enriched = generate_metadata_for_evaluation_metadata_evidence(row, query, chunk_table)\n",
    "            if enriched:\n",
    "                row = enriched\n",
    "                success += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "            enriched_batch.append(clean_json(row))\n",
    "\n",
    "        if enriched_batch:\n",
    "            enriched_data.extend(enriched_batch)\n",
    "            \n",
    "            # Incrementally save after each batch\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(enriched_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            logging.info(f\"Processed batch {i//batch_size + 1}, added {len(enriched_batch)} rows. Total so far: {len(enriched_data)}\")\n",
    "\n",
    "        time.sleep(2)  # prevent overloading LLM/API\n",
    "\n",
    "    print(f\"[✓] Metadata generation complete. Success: {success}, Skipped: {skipped}, Total: {total}\")\n",
    "    logging.info(f\"Final counts — Success: {success}, Skipped: {skipped}, Total: {total}\")\n",
    "\n",
    "    return enriched_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Initialize DB\n",
    "LANCE_DB_PATH = \"./lancedb\"\n",
    "db = connect(LANCE_DB_PATH)\n",
    "eval_table = db.open_table(\"evaluations\")\n",
    "chunk_table = db.open_table(\"chunks\")\n",
    "enriched_data= generate_evaluation_metadata(eval_table, chunk_table, output_file=\"all_evaluations_metadata2.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Structured Information Extraction\n",
    "\n",
    "\n",
    "### Standard Questions\n",
    "\n",
    "```{python} \n",
    "# Define the list of experts on impact - outcome - organisation\n",
    "q_experts = [\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Attaining favorable protection environments---: i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Realizing rights in safe environments---: i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Empowering communities and achieving gender equality--- : i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Securing durable solutions--- : i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n",
    "\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: ---Access to territory registration and documentation ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Status determination ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Protection policy and law---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Gender-based violence ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Child protection ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Safety and access to justice ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Community engagement and women's empowerment ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Well-being and basic needs ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Sustainable housing and settlements ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Healthy lives---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Education ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Clean water sanitation and hygiene ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Self-reliance, Economic inclusion, and livelihoods ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Voluntary repatriation and sustainable reintegration ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Resettlement and complementary pathways---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Local integration and other local solutions ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\", \n",
    "\n",
    "\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Systems and processes, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\",\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Operational support and supply chain, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to People and culture, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to External engagement and resource mobilization, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n",
    "   \"<s> [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Leadership and governance, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" \n",
    "]\n",
    "\n",
    "# Predefined knowledge extraction questions\n",
    "q_questions = [\n",
    "    \" List, as bullet points, all findings and evidences in relation to your specific area of expertise and focus. \",\n",
    "    \" Explain, in relation to your specific area of expertise and focus, what are the root causes for the situation. \" ,\n",
    "    \" Explain, in relation to your specific area of expertise and focus, what are the main risks and difficulties here described. \",\n",
    "    \" Explain, in relation to your specific area of expertise and focus, what what can be learnt. \",\n",
    "    \" List, as bullet points, all recommendations made in relation to your specific area of expertise and focus. \"#,\n",
    "    # \"Indicate if mentionnend what resource will be required to implement the recommendations made in relation to your specific area of expertise and focus. \",\n",
    "    # \"List, as bullet points, all recommendations made in relation to your specific area of expertise and focus that relates to topics  or activities recommended to be discontinued. \",\n",
    "    # \"List, as bullet points, all recommendations made in relation to your specific area of expertise and focus that relates to topics or activities recommended to be scaled up. \" \n",
    "    # Add more questions here...\n",
    "]\n",
    "\n",
    "## Additional instructions!\n",
    "q_instr = \"\"\"\n",
    "</s>\n",
    "[INST]  \n",
    "Keep your answer grounded in the facts of the contexts. \n",
    "If the contexts do not contain the facts to answer the QUESTION, return {NONE} \n",
    "Be concise in the response and  when relevant include precise citations from the contexts. \n",
    "[/INST] \n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "###  Q&A Extraction\n",
    "\n",
    "```{python} \n",
    "qa_questions = [\n",
    "    \"What was the intervention type?\",\n",
    "    \"What outcomes were observed?\",\n",
    "    \"What population was targeted?\",\n",
    "    \"What geographic area was covered?\",\n",
    "    \"How strong is the evidence?\",\n",
    "]\n",
    "\n",
    "def generate_qas(text):\n",
    "    prompt = f\"\"\"Extract answers to the following questions from the evaluation:\n",
    "    {json.dumps(qa_questions)}\n",
    "    \n",
    "    Text: {text[:3000]}\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "df_docs[\"qa\"] = df_docs[\"text\"].apply(generate_qas)\n",
    "```\n",
    "\n",
    "### Hybrid Search in LanceDB\n",
    "\n",
    "```{python} \n",
    "# Sample hybrid search query\n",
    "query = \"What works best to improve health outcomes for displaced persons?\"\n",
    "\n",
    "query_embedding = get_text_embedding(query)\n",
    "results = table.search(query_embedding).limit(5).to_list()\n",
    "\n",
    "for result in results:\n",
    "    print(result['metadata'])\n",
    "    print(result['text'][:500])\n",
    "```\n",
    "\n",
    "\n",
    "```{python} \n",
    "def query_evidence(question: str, table: lancedb.db.LanceTable) -> Dict:\n",
    "    \"\"\"Enhanced query with hybrid search and evidence grading\"\"\"\n",
    "    try:\n",
    "        # Hybrid search\n",
    "        results = hybrid_search(table, question, limit=7)\n",
    "        \n",
    "        if results.empty:\n",
    "            return {\"answer\": \"No relevant evidence found.\", \"sources\": []}\n",
    "        \n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1} (Relevance: {row.get('combined_score', 0):.2f}):\\n{row['text']}\\n\"\n",
    "            for i, row in results.iterrows()\n",
    "        ])\n",
    "        \n",
    "        # Evidence-based answer generation\n",
    "        prompt = f\"\"\"\n",
    "        You are an evidence specialist answering questions about IOM programs.\n",
    "        Use ONLY the provided context from evaluation reports.\n",
    "        For each claim in your answer, cite the document number it came from.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Provide:\n",
    "        1. A direct answer to the question\n",
    "        2. Strength of evidence (High/Medium/Low)\n",
    "        3. Any limitations or caveats\n",
    "        4. List of sources with relevance scores\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=config.chat_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        sources = [\n",
    "            {\"url\": url, \"score\": score}\n",
    "            for url, score in zip(results['url'], results.get('combined_score', 0))\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"search_scores\": results[['_distance', '_score', 'combined_score']].to_dict()\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Query error: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{python} \n",
    "def extract_structured_info(table, iom_framework):\n",
    "    \"\"\"Extract structured information from reports using the IOM Results Framework\"\"\"\n",
    "    # Generate questions based on the IOM framework\n",
    "    questions = generate_questions_from_framework(iom_framework)\n",
    "    \n",
    "    # Store extracted information\n",
    "    extracted_data = []\n",
    "    \n",
    "    # Process each question\n",
    "    for question in questions:\n",
    "        print(f\"Processing question: {question}\")\n",
    "        \n",
    "        # Search for relevant chunks\n",
    "        results = table.search(generate_embeddings([question])[0]).limit(10).to_pandas()\n",
    "        \n",
    "        # Combine relevant chunks as context\n",
    "        context = \"\\n\\n\".join(results[\"text\"].tolist())\n",
    "        \n",
    "        # Use Azure OpenAI to extract structured answer\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following evaluation report excerpts, answer the question with structured information.\n",
    "        Provide your answer in JSON format with the following structure:\n",
    "        {{\n",
    "            \"question\": \"the question being asked\",\n",
    "            \"answer\": \"the concise answer\",\n",
    "            \"intervention_type\": \"type of intervention mentioned\",\n",
    "            \"population\": \"target population mentioned\",\n",
    "            \"outcome\": \"outcome measured\",\n",
    "            \"geography\": \"geographic location if mentioned\",\n",
    "            \"evidence_strength\": \"strength of evidence (high/medium/low)\"\n",
    "        }}\n",
    "\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=config[\"azure_openai_chat_deployment\"],\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=config[\"temperature\"],\n",
    "            max_tokens=config[\"max_tokens\"]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            answer = json.loads(response.choices[0].message.content)\n",
    "            answer[\"source_urls\"] = results[\"url\"].unique().tolist()\n",
    "            extracted_data.append(answer)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse answer for question: {question}\")\n",
    "    \n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "def generate_questions_from_framework(framework_df):\n",
    "    \"\"\"Generate questions based on the IOM Results Framework\"\"\"\n",
    "    questions = []\n",
    "    \n",
    "    # Example questions based on common evaluation themes\n",
    "    for _, row in framework_df.iterrows():\n",
    "        questions.extend([\n",
    "            f\"What interventions has IOM implemented to achieve {row['Objective']}?\",\n",
    "            f\"What evidence exists for the effectiveness of interventions targeting {row['Outcome']}?\",\n",
    "            f\"What populations have been targeted by interventions aiming for {row['Indicator']}?\",\n",
    "            f\"What geographic areas have seen interventions related to {row['Objective']}?\",\n",
    "            f\"What methodologies have been used to evaluate interventions for {row['Outcome']}?\"\n",
    "        ])\n",
    "    \n",
    "    # Add some general evaluation questions\n",
    "    questions.extend([\n",
    "        \"What are the most effective interventions for migrant livelihood improvement?\",\n",
    "        \"What evidence exists for cash-based interventions in migration contexts?\",\n",
    "        \"What are common challenges in implementing migration programs?\",\n",
    "        \"What evaluation methodologies are most commonly used in IOM evaluations?\",\n",
    "        \"What gaps exist in the evidence base for migration interventions?\"\n",
    "    ])\n",
    "    \n",
    "    return list(set(questions))  # Remove duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def hybrid_search(table: lancedb.db.LanceTable, query: str, limit: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Perform hybrid (vector + full-text) search\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = generate_embeddings_batch([query])[0]\n",
    "    \n",
    "    # Perform hybrid search\n",
    "    results = table.search(query_embedding, query_string=query)\\\n",
    "                 .limit(limit)\\\n",
    "                 .to_pandas()\n",
    "    \n",
    "    # Score normalization (simple example)\n",
    "    if not results.empty:\n",
    "        max_vector_score = results[\"_distance\"].max()\n",
    "        max_fts_score = results[\"_score\"].max()\n",
    "        \n",
    "        if max_vector_score > 0 and max_fts_score > 0:\n",
    "            results[\"combined_score\"] = (\n",
    "                0.7 * (results[\"_distance\"] / max_vector_score) +\n",
    "                0.3 * (results[\"_score\"] / max_fts_score)\n",
    "            )\n",
    "            results = results.sort_values(\"combined_score\", ascending=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_structured_info(table: lancedb.db.LanceTable, iom_framework: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced information extraction with hybrid search\"\"\"\n",
    "    questions = generate_questions_from_framework(iom_framework)\n",
    "    extracted_data = []\n",
    "    \n",
    "    for question in questions:\n",
    "        try:\n",
    "            # Hybrid search for relevant chunks\n",
    "            results = hybrid_search(table, question, limit=15)\n",
    "            \n",
    "            if results.empty:\n",
    "                continue\n",
    "                \n",
    "            context = \"\\n\\n\".join(results[\"text\"].tolist())\n",
    "            sources = results[\"url\"].unique().tolist()\n",
    "            \n",
    "            # Structured extraction prompt\n",
    "            prompt = f\"\"\"\n",
    "            Extract structured information from this evaluation report context to answer the question.\n",
    "            Return ONLY valid JSON with this structure:\n",
    "            {{\n",
    "                \"question\": \"the question\",\n",
    "                \"answer\": \"concise answer\",\n",
    "                \"intervention_type\": [\"type1\", \"type2\"],\n",
    "                \"population\": [\"group1\", \"group2\"],\n",
    "                \"outcome\": [\"outcome1\", \"outcome2\"],\n",
    "                \"geography\": [\"location1\", \"location2\"],\n",
    "                \"evidence_strength\": \"high/medium/low\",\n",
    "                \"source_urls\": [\"url1\", \"url2\"]\n",
    "            }}\n",
    "            \n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=config.chat_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                response_format={ \"type\": \"json_object\" }\n",
    "            )\n",
    "            \n",
    "            answer = json.loads(response.choices[0].message.content)\n",
    "            answer[\"source_urls\"] = sources\n",
    "            extracted_data.append(answer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Cross-Evaluation Analysis\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Generate Actionable and Generalizable Insights  \n",
    "\n",
    "One key challenge is How to generalize the findings from an evaluation from one place to another one? The [Generalizability Framework](https://ssir.org/articles/entry/the_generalizability_puzzle) provides some insights on how to do that.\n",
    "\n",
    "\n",
    "To implement this we will generate insights using AI-enabled Q&A on all previous Q&A:\n",
    "\n",
    "```{python} \n",
    "def generate_insights(df):\n",
    "    # Add your insight generation logic here\n",
    "    return df\n",
    "\n",
    "df = generate_insights(df)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Identify Patterns and Gaps\n",
    "\n",
    "Identify patterns and gaps in the data:\n",
    "\n",
    "```{python} \n",
    "def identify_patterns(df):\n",
    "    # Add your pattern identification logic here\n",
    "    return df\n",
    "\n",
    "df = identify_patterns(df)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{python} \n",
    "def generate_deliverables(df):\n",
    "    # Generate Q&A dataset\n",
    "    qa_dataset = df[['question', 'answer']]\n",
    "\n",
    "    # Generate synthesis report\n",
    "    synthesis_report = df.describe()\n",
    "\n",
    "    # Generate visual evidence map\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x='outcome', y='population', size='sample_size')\n",
    "    plt.title('Visual Evidence Map')\n",
    "    plt.show()\n",
    "\n",
    "    return qa_dataset, synthesis_report\n",
    "\n",
    "qa_dataset, synthesis_report = generate_deliverables(df)\n",
    "```\n",
    "\n",
    "Visualize Patterns & Gaps\n",
    "\n",
    "```{python} \n",
    "# Convert QA to structured fields (intervention, outcome, population, etc.)\n",
    "qa_df = pd.json_normalize(df_docs[\"qa\"].apply(json.loads))\n",
    "\n",
    "# Bubble Map Example\n",
    "fig = px.scatter(qa_df, x=\"geography\", y=\"outcome\",\n",
    "                 size=\"sample_size\", color=\"intervention\",\n",
    "                 hover_name=\"file_name\",\n",
    "                 title=\"Evidence Bubble Map\")\n",
    "fig.show()\n",
    "\n",
    "# Heatmap Example\n",
    "heatmap_df = pd.crosstab(qa_df[\"intervention\"], qa_df[\"outcome\"])\n",
    "sns.heatmap(heatmap_df, annot=True, cmap=\"coolwarm\")\n",
    "```\n",
    "\n",
    "```{python} \n",
    "def create_interactive_visualizations(extracted_data: pd.DataFrame):\n",
    "    \"\"\"Enhanced visualization functions\"\"\"\n",
    "    # Prepare data\n",
    "    df = extracted_data.explode(\"source_urls\")\n",
    "    \n",
    "    # Evidence Strength Distribution\n",
    "    strength_dist = df['evidence_strength'].value_counts().reset_index()\n",
    "    fig1 = px.bar(\n",
    "        strength_dist,\n",
    "        x='evidence_strength',\n",
    "        y='count',\n",
    "        title='Distribution of Evidence Strength'\n",
    "    )\n",
    "    \n",
    "    # Interventions by Geography\n",
    "    fig2 = px.treemap(\n",
    "        df,\n",
    "        path=['geography', 'intervention_type'],\n",
    "        title='Interventions by Geographic Region'\n",
    "    )\n",
    "    \n",
    "    # Evidence Timeline (if dates available)\n",
    "    if 'date' in df.columns:\n",
    "        fig3 = px.line(\n",
    "            df.groupby('date').size().reset_index(name='count'),\n",
    "            x='date',\n",
    "            y='count',\n",
    "            title='Evidence Publication Timeline'\n",
    "        )\n",
    "    else:\n",
    "        fig3 = None\n",
    "    \n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{python} \n",
    "def visualize_evidence_map(extracted_data):\n",
    "    \"\"\"Create interactive visualizations of the evidence map\"\"\"\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    df = extracted_data.explode(\"source_urls\")\n",
    "    \n",
    "    # Bubble map: Interventions by outcome and evidence strength\n",
    "    fig1 = px.scatter(\n",
    "        df, \n",
    "        x=\"outcome\", \n",
    "        y=\"intervention_type\", \n",
    "        size=\"evidence_strength\",  # This would need to be mapped to numeric values\n",
    "        color=\"population\",\n",
    "        hover_name=\"answer\",\n",
    "        title=\"Evidence Map: Interventions by Outcome and Population\"\n",
    "    )\n",
    "    fig1.update_layout(height=800)\n",
    "    \n",
    "    # Heatmap: Evidence concentration by intervention and outcome\n",
    "    heatmap_data = df.groupby(['intervention_type', 'outcome']).size().unstack().fillna(0)\n",
    "    fig2 = px.imshow(\n",
    "        heatmap_data,\n",
    "        labels=dict(x=\"Outcome\", y=\"Intervention Type\", color=\"Number of Studies\"),\n",
    "        title=\"Evidence Concentration Heatmap\"\n",
    "    )\n",
    "    \n",
    "    # Gap map: Missing evidence\n",
    "    all_interventions = df['intervention_type'].unique()\n",
    "    all_outcomes = df['outcome'].unique()\n",
    "    complete_grid = pd.MultiIndex.from_product([all_interventions, all_outcomes], names=['intervention_type', 'outcome'])\n",
    "    gap_data = df.groupby(['intervention_type', 'outcome']).size().reindex(complete_grid, fill_value=0).reset_index()\n",
    "    gap_data['has_evidence'] = gap_data[0] > 0\n",
    "    \n",
    "    fig3 = px.scatter(\n",
    "        gap_data,\n",
    "        x=\"outcome\",\n",
    "        y=\"intervention_type\",\n",
    "        color=\"has_evidence\",\n",
    "        title=\"Evidence Gap Map (Red = Missing Evidence)\"\n",
    "    )\n",
    "    \n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "def generate_synthesis_report(extracted_data):\n",
    "    \"\"\"Generate a narrative synthesis report of findings\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an evaluation specialist analyzing evidence from IOM evaluation reports.\n",
    "    Below is structured data extracted from multiple evaluation reports:\n",
    "    \n",
    "    {extracted_data.to_json()}\n",
    "    \n",
    "    Write a comprehensive synthesis report that:\n",
    "    1. Summarizes key findings across interventions\n",
    "    2. Identifies areas with strong evidence\n",
    "    3. Highlights evidence gaps\n",
    "    4. Provides recommendations for future evaluations\n",
    "    5. Suggests high-priority research areas\n",
    "    \n",
    "    Structure your report with clear sections and bullet points for readability.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=config[\"azure_openai_chat_deployment\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,  # Slightly more creative for synthesis\n",
    "        max_tokens=3000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Save the deliverables to files:\n",
    "\n",
    "```{python} \n",
    "qa_dataset.to_csv('qa_dataset.csv', index=False)\n",
    "synthesis_report.to_csv('synthesis_report.csv')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusions - and potential extension...\n",
    "\n",
    "* Web interface (Streamlit, Gradio, etc.)\n",
    "\n",
    "* Periodic syncing with new evaluations via web scraping\n",
    "\n",
    "* Integration with Hugging Face for fine-tuning a summarization or Q&A model\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
