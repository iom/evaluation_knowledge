{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from evaluation_knowledge.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Assisted Evaluation Evidence Mapping\n",
    "\n",
    ">  Instead of spending hours searching through reports asking \"_what worked in similar contexts?_\", equip program managers with instant, evidence-based answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "An **Evidence Map** is a structured, visual tool that organizes what we know—and don’t know—about programs, policies, and interventions. Think of it as a research and evaluation landscape that helps understand how the evidence available is distributed against a framework of reference: for example, the Sustainable Development Goals:\n",
    "\n",
    "- Which interventions have been evaluated  \n",
    "- Where they were implemented  \n",
    "- What outcomes were observed  \n",
    "- Where critical knowledge gaps remain  \n",
    "\n",
    "This approach is especially valuable for evidence-informed project design, particularly when time or resources limit the ability to read through hundreds of individual evaluation reports.\n",
    "\n",
    "More than just tagging content, the system smartly extracts the 'why' behind program success - not just outcomes, but mechanisms, required conditions, and implementation factors. This means better program design, faster proposal development, and fewer repeated mistakes. Evaluations become a __living knowledge base__ that actually gets used. This project shall enhance the speed and quality of learning. Instead of each organization reinventing the wheel or making the same mistakes, we'd have:\n",
    "\n",
    " * Real-time access to \"what worked where and why\"\n",
    " * Faster identification of promising approaches to test\n",
    " * Better understanding of when and how to adapt programs\n",
    "\n",
    "This tool can not only inform future evaluation design, guide strategic planning, and support the development of robust, evidence-based project proposals and strategic plan, but also could fundamentally change how evidence flows through the humanitarian system - from slow, siloed learning to rapid, networked knowledge sharing.\n",
    "\n",
    "## Some examples of Evidence Maps:\n",
    "\n",
    "* [Evidence and Gap Maps relating to Sustainable Development Goals](https://www.sdgsynthesiscoalition.org/sites/default/files/2024-10/Map%20of%20Maps%20on%20SDGs%20in%20LMICs_05122024.html)\n",
    "\n",
    "* [Known from birth: Generating and using evidence to strengthen birth registration systems in Africa (UNICEF)](https://www.unicef.org/innocenti/media/3861/file/CA-BirthEviMap-Final-14.09.2023.pdf)\n",
    "\n",
    "* Child and Adolescent Mental Health and Psychosocial Support Interventions: An evidence and gap map of low- and middle-income countries ([Report](https://onlinelibrary.wiley.com/doi/10.1002/cl2.1349?msockid=0f883b2b378764a726682f9536af656c))\n",
    "\n",
    "* Youth Futures Foundation Evidence & Gap Map on Interventions to Increase Youth Employment ([Map](https://youthfuturesfoundation.org/wp-content/uploads/2025/02/Youth-Futures-Evidence-and-Gap-Map-updated-Jan-2025.html); [Report](https://youthfuturesfoundation.org/wp-content/uploads/2024/11/Youth-Futures-Foundation-Evidence-and-Gap-Map-Effectiveness-of-Interventions-Technical-Report-November-2024.pdf))\n",
    "\n",
    "* [Evidence map by the UNSDG System-Wide Evaluation Office (SWEO) mapping UN evaluations against the 2024 Quadrennial Comprehensive Policy Review (QCPR)](https://www.sdgsynthesiscoalition.org/sites/default/files/2024-10/UNSWE_Interactive%20Evaluation%20Evidence%20Map_QCPR_coverage_v1.0.html)\n",
    "\n",
    "* [UNICEF Evidence Map example](https://evaluationreports.unicef.org/app/evaluation-evidence-gap-map.html)).\n",
    "\n",
    "\n",
    "\n",
    "## About this project\n",
    "\n",
    "Evidence Maps are relatively established tools that started being common before the popularization of LLMs and AI in general. Their production generally entails a large amount of staff-hours spent on reviewing documentation and \"tagging\" it, with more or less articulated arrangements for source selection and cross-verification of the human-led classification. Recent practice in evidence mapping focuses on the automation of different steps of the map production process via AI.\n",
    "\n",
    "The goal of this exercise is to create an AI-powered open source application capable of accurately \"\"tagging\" evaluation reports against the [IOM Strategic Result Framework](https://www.iom.int/iom-strategic-results-framework-srf) and the [Global Compact on Migration](https://www.iom.int/global-compact-migration), two important tools that guide the action of IOM and other entities in the humanitarian and development space. \n",
    "\n",
    "Rather than a fully automated tagging tool, the instrument will be used in the context of a structured and efficient human-machine collaboration protocol whereby the relative strengths of each are leveraged. This initiative aims at producing a fully reproducible solution that is not only scalable, but also easy to repurpose for other applications in the humanitarian and development space.\n",
    "\n",
    "The goal of this exercise is to inform future evaluation design, guide strategic planning, and support the development of robust, evidence-based project proposals and strategic plan.\n",
    "\n",
    "In addition, such AI-enhanced approach shall help to manage evaluation output as  __[\"LivingEvidence\"](https://www.aliveevidence.org/)__ - aka looking at knowledge synthesis as an ongoing rather than static endeavor - that can improve the timeliness of recommendation updates and reduce the knowledge-to-practice gap. \n",
    "\n",
    "Last The approach used here is also tackling the __[Evidence Generalization Problem](https://ssir.org/articles/entry/the_generalizability_puzzle)__, which refers to the challenge of applying findings from one context to another. By systematically mapping evidence across diverse contexts, we can better understand how and why certain interventions work in specific settings, and identify the conditions under which they are most effective.\n",
    "\n",
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "To ensure relevance, we begin by clarifying the scope and purpose of the mapping:\n",
    "\n",
    "- **What types of interventions are we assessing?** (IOM programs, policies, and strategies, e.g., cash-based interventions, health services, community engagement)\n",
    "\n",
    "- **Who are the target populations?**  (e.g., migrants, displaced persons, host communities)  \n",
    "\n",
    "- **What outcomes matter most?**  (e.g., livelihood improvements, health outcomes, social integration - Aligned with the [IOM Strategic Results Framework](https://www.iom.int/iom-strategic-results-framework-srf))  \n",
    "\n",
    "- **Who is the audience for this map?**   (e.g., policymakers, funders, researchers, program managers, donors)  \n",
    "\n",
    "- **What are our key learning questions?**  (e.g., “What works best to maximize impact an effectivness?”)  \n",
    "\n",
    "- **What level of evidence is required?**  (e.g., RCTs, quasi-experiments, observational studies)  \n",
    "\n",
    "- **What variables are we tracking?** (e.g., intervention type, target group, outcomes, geography)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Building the Knowledge Base\n",
    "\n",
    "We have compiled a list of all publicly available [IOM Evaluation Reports](https://evaluation.iom.int/evaluation-search-pdf).\n",
    "\n",
    "Each report will be analyzed to generate a structured **metadata record**, including:\n",
    "\n",
    "- **What**: Title, summary, full-text link, evaluation type (formative, summative, impact), scope (strategy, policy, thematic, program, or project), and geographic coverage  \n",
    "- **Who**: Conducting entity (IOM internal vs. external evaluators)  \n",
    "- **How**: Methodology, study design, sample size, and data collection techniques  \n",
    "\n",
    "These metadata and full-text documents will be convert the content of each report into a **embeddings vector database**, enabling fast, flexible, and AI-enhanced retrieval using advanced tools like [Hybrid Search](https://docs.lancedb.com/core/hybrid-search).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Structured Information Extraction\n",
    "\n",
    "Instead of just summarizing each evaluation report, we use AI to answer these same questions for every single evaluation. This creates comparable data across all studies.\n",
    "\n",
    "We will create a set of plain-language questions, reflecting the entire IOM Results Framework.\n",
    "Using AI tools, we will extract consistent and comparable data from each report:\n",
    "\n",
    "- **Program details** (what was implemented)  \n",
    "- **Context** (where and with whom)  \n",
    "- **Design** (how it was studied)  \n",
    "- **Findings** (what results were observed)  \n",
    "- **Strength of evidence** (how reliable the findings are)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Cross-Evaluation Analysis\n",
    "\n",
    "We will then run those questions through the vector database to generate answers based first on each evaluation.\n",
    "\n",
    "This will allow to categorize the data within a structured framework:\n",
    "\n",
    "1. **By intervention type** (e.g., skills training, psychosocial support)  \n",
    "2. **By measured outcome** (e.g., employment, resilience, community cohesion)  \n",
    "3. **By population** (e.g., migrants in transit, returnees, host communities)  \n",
    "4. **By evidence quality** (e.g., robust vs. exploratory studies)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Generate Actionable and Generalizable Insights \n",
    "\n",
    "One key challenge is How to generalize the findings from an evaluation from one place to another one? The [Generalizability Framework](https://ssir.org/articles/entry/the_generalizability_puzzle) provides some insights on how to do that.\n",
    "\n",
    "We will use the same questions to generate a **Q&A dataset** that can be used to answer the same questions across all evaluations, aka the full corpus of Q&A previously generated, therefore accounting for all evidences that were gathered across all evaluations. \n",
    "\n",
    "This will allow us to quickly assess the evidence base and identify key insights, such as: _ \"What types of interventions are most effective in improving livelihood outcomes for migrants in urban settings?\"_\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Identify Patterns and Gaps\n",
    "\n",
    "We’ll present the results across objectives, outcome, population and geography using intuitive, interactive visualizations:\n",
    "\n",
    "- **Bubble maps** (bubble size = number of studies or sample size)  \n",
    "- **Heatmaps** (showing concentration of evidence by topic or geography)  \n",
    "- **Gap maps** (highlighting under-researched areas)\n",
    "\n",
    "The evidence system will allow us to quickly highlight:\n",
    "\n",
    "- ✅ Areas with strong, consistent evidence  \n",
    "- ⚠️ Topics with mixed or conflicting findings  \n",
    "- ❌ Critical gaps where no evidence exists  \n",
    "\n",
    "*Example Insight:*  \n",
    "> “Mentoring programs show consistent positive results for urban migrant youth, but there’s limited evidence for rural populations.”\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "The final deliverables will include:\n",
    "\n",
    "- A Q&A dataset that can be used:\n",
    "  - As a reference for [content curation and evaluation](https://api.labelstud.io/tutorials/tutorials/evaluate-llm-responses)  \n",
    "  - As a [knowledge base](https://docs.crewai.com/concepts/knowledge) for AI-enhanced project proposal systems  \n",
    "  - As a training dataset for fine-tuning small language models via [Hugging Face](https://huggingface.co/)  \n",
    "\n",
    "- A synthesis report identifying:\n",
    "  - Research priorities  \n",
    "  - High-risk areas for intervention  \n",
    "  - Recommendations for future evaluations  \n",
    "\n",
    "- A searchable online **visual evidence map** for ongoing use by IOM teams that would allow to see evidence according to both SRF and GCM frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module implements a Literate Programming approach, which means:\n",
    "\n",
    "* __Documentation__: Each handler serves as comprehensive documentation.\n",
    "* __Code Reference__: The notebooks contain the actual implementation code.\n",
    "* __Communication Tool__: They facilitate discussions with data providers about discrepancies or inconsistencies.\n",
    "\n",
    "\n",
    "If you are new to using `nbdev`, here are [some useful pointers to get you started](https://nbdev.fast.ai/tutorials/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install evaluation_knowledge in Development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "# make sure evaluation_knowledge package is installed in development mode\n",
    "$ pip install -e .\n",
    "\n",
    "# make changes under nbs/ directory\n",
    "# ...\n",
    "\n",
    "# compile to have changes apply to evaluation_knowledge\n",
    "$ nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install latest from the GitHub [repository][repo]:\n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/iom/evaluation_knowledge.git\n",
    "```\n",
    "\n",
    "or from [conda][conda]\n",
    "\n",
    "```sh\n",
    "$ conda install -c iom evaluation_knowledge\n",
    "```\n",
    "\n",
    "or from [pypi][pypi]\n",
    "\n",
    "\n",
    "```sh\n",
    "$ pip install evaluation_knowledge\n",
    "```\n",
    "\n",
    "\n",
    "[repo]: https://github.com/iom/evaluation_knowledge\n",
    "[docs]: https://iom.github.io/evaluation_knowledge/\n",
    "[pypi]: https://pypi.org/project/evaluation_knowledge/\n",
    "[conda]: https://anaconda.org/iom/evaluation_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation can be found hosted on this GitHub [repository][repo]'s [pages][docs]. Additionally you can find package manager specific guidelines on [conda][conda] and [pypi][pypi] respectively.\n",
    "\n",
    "[repo]: https://github.com/iom/evaluation_knowledge\n",
    "[docs]: https://iom.github.io/evaluation_knowledge/\n",
    "[pypi]: https://pypi.org/project/evaluation_knowledge/\n",
    "[conda]: https://anaconda.org/iom/evaluation_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the documentation of your library\n",
    "\n",
    "2. Run the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
