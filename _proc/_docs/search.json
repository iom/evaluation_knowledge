[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "",
    "text": "An Evidence Map is a structured, visual tool that organizes what we know—and don’t know—about programs, policies, and interventions. Think of it as a research and evaluation landscape that helps understand how the evidence available is distributed against a framework of reference: for example, the Sustainable Development Goals:\n\nWhich interventions have been evaluated\n\nWhere they were implemented\n\nWhat outcomes were observed\n\nWhere critical knowledge gaps remain\n\nThis approach is especially valuable for evidence-informed project design, particularly when time or resources limit the ability to read through hundreds of individual evaluation reports.\nMore than just tagging content, the system smartly extracts the ‘why’ behind program success - not just outcomes, but mechanisms, required conditions, and implementation factors. This means better program design, faster proposal development, and fewer repeated mistakes. Evaluations become a living knowledge base that actually gets used. This project shall enhance the speed and quality of learning. Instead of each organization reinventing the wheel or making the same mistakes, we’d have:\n\nReal-time access to “what worked where and why”\nFaster identification of promising approaches to test\nBetter understanding of when and how to adapt programs\n\nThis tool can not only inform future evaluation design, guide strategic planning, and support the development of robust, evidence-based project proposals and strategic plan, but also could fundamentally change how evidence flows through the humanitarian system - from slow, siloed learning to rapid, networked knowledge sharing.",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "",
    "text": "An Evidence Map is a structured, visual tool that organizes what we know—and don’t know—about programs, policies, and interventions. Think of it as a research and evaluation landscape that helps understand how the evidence available is distributed against a framework of reference: for example, the Sustainable Development Goals:\n\nWhich interventions have been evaluated\n\nWhere they were implemented\n\nWhat outcomes were observed\n\nWhere critical knowledge gaps remain\n\nThis approach is especially valuable for evidence-informed project design, particularly when time or resources limit the ability to read through hundreds of individual evaluation reports.\nMore than just tagging content, the system smartly extracts the ‘why’ behind program success - not just outcomes, but mechanisms, required conditions, and implementation factors. This means better program design, faster proposal development, and fewer repeated mistakes. Evaluations become a living knowledge base that actually gets used. This project shall enhance the speed and quality of learning. Instead of each organization reinventing the wheel or making the same mistakes, we’d have:\n\nReal-time access to “what worked where and why”\nFaster identification of promising approaches to test\nBetter understanding of when and how to adapt programs\n\nThis tool can not only inform future evaluation design, guide strategic planning, and support the development of robust, evidence-based project proposals and strategic plan, but also could fundamentally change how evidence flows through the humanitarian system - from slow, siloed learning to rapid, networked knowledge sharing.",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#some-examples-of-evidence-maps",
    "href": "index.html#some-examples-of-evidence-maps",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "Some examples of Evidence Maps:",
    "text": "Some examples of Evidence Maps:\n\nEvidence and Gap Maps relating to Sustainable Development Goals\nKnown from birth: Generating and using evidence to strengthen birth registration systems in Africa (UNICEF)\nChild and Adolescent Mental Health and Psychosocial Support Interventions: An evidence and gap map of low- and middle-income countries (Report)\nYouth Futures Foundation Evidence & Gap Map on Interventions to Increase Youth Employment (Map; Report)\nEvidence map by the UNSDG System-Wide Evaluation Office (SWEO) mapping UN evaluations against the 2024 Quadrennial Comprehensive Policy Review (QCPR)\nUNICEF Evidence Map example).",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "About this project",
    "text": "About this project\nEvidence Maps are relatively established tools that started being common before the popularization of LLMs and AI in general. Their production generally entails a large amount of staff-hours spent on reviewing documentation and “tagging” it, with more or less articulated arrangements for source selection and cross-verification of the human-led classification. Recent practice in evidence mapping focuses on the automation of different steps of the map production process via AI.\nThe goal of this exercise is to create an AI-powered open source application capable of accurately ““tagging” evaluation reports against the IOM Strategic Result Framework and the Global Compact on Migration, two important tools that guide the action of IOM and other entities in the humanitarian and development space.\nRather than a fully automated tagging tool, the instrument will be used in the context of a structured and efficient human-machine collaboration protocol whereby the relative strengths of each are leveraged. This initiative aims at producing a fully reproducible solution that is not only scalable, but also easy to repurpose for other applications in the humanitarian and development space.\nThe goal of this exercise is to inform future evaluation design, guide strategic planning, and support the development of robust, evidence-based project proposals and strategic plan.\nIn addition, such AI-enhanced approach shall help to manage evaluation output as “LivingEvidence” - aka looking at knowledge synthesis as an ongoing rather than static endeavor - that can improve the timeliness of recommendation updates and reduce the knowledge-to-practice gap.\nLast The approach used here is also tackling the Evidence Generalization Problem, which refers to the challenge of applying findings from one context to another. By systematically mapping evidence across diverse contexts, we can better understand how and why certain interventions work in specific settings, and identify the conditions under which they are most effective.",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#approach",
    "href": "index.html#approach",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "Approach",
    "text": "Approach\nTo ensure relevance, we begin by clarifying the scope and purpose of the mapping:\n\nWhat types of interventions are we assessing? (IOM programs, policies, and strategies, e.g., cash-based interventions, health services, community engagement)\nWho are the target populations? (e.g., migrants, displaced persons, host communities)\nWhat outcomes matter most? (e.g., livelihood improvements, health outcomes, social integration - Aligned with the IOM Strategic Results Framework)\nWho is the audience for this map? (e.g., policymakers, funders, researchers, program managers, donors)\nWhat are our key learning questions? (e.g., “What works best to maximize impact an effectivness?”)\nWhat level of evidence is required? (e.g., RCTs, quasi-experiments, observational studies)\nWhat variables are we tracking? (e.g., intervention type, target group, outcomes, geography)\n\n\n\nStep 1: Building the Knowledge Base\nWe have compiled a list of all publicly available IOM Evaluation Reports.\nEach report will be analyzed to generate a structured metadata record, including:\n\nWhat: Title, summary, full-text link, evaluation type (formative, summative, impact), scope (strategy, policy, thematic, program, or project), and geographic coverage\n\nWho: Conducting entity (IOM internal vs. external evaluators)\n\nHow: Methodology, study design, sample size, and data collection techniques\n\nThese metadata and full-text documents will be convert the content of each report into a embeddings vector database, enabling fast, flexible, and AI-enhanced retrieval using advanced tools like Hybrid Search.\n\n\n\nStep 2: Structured Information Extraction\nInstead of just summarizing each evaluation report, we use AI to answer these same questions for every single evaluation. This creates comparable data across all studies.\nWe will create a set of plain-language questions, reflecting the entire IOM Results Framework. Using AI tools, we will extract consistent and comparable data from each report:\n\nProgram details (what was implemented)\n\nContext (where and with whom)\n\nDesign (how it was studied)\n\nFindings (what results were observed)\n\nStrength of evidence (how reliable the findings are)\n\n\n\n\nStep 3: Cross-Evaluation Analysis\nWe will then run those questions through the vector database to generate answers based first on each evaluation.\nThis will allow to categorize the data within a structured framework:\n\nBy intervention type (e.g., skills training, psychosocial support)\n\nBy measured outcome (e.g., employment, resilience, community cohesion)\n\nBy population (e.g., migrants in transit, returnees, host communities)\n\nBy evidence quality (e.g., robust vs. exploratory studies)\n\n\n\n\nStep 4: Generate Actionable and Generalizable Insights\nOne key challenge is How to generalize the findings from an evaluation from one place to another one? The Generalizability Framework provides some insights on how to do that.\nWe will use the same questions to generate a Q&A dataset that can be used to answer the same questions across all evaluations, aka the full corpus of Q&A previously generated, therefore accounting for all evidences that were gathered across all evaluations.\nThis will allow us to quickly assess the evidence base and identify key insights, such as: _ “What types of interventions are most effective in improving livelihood outcomes for migrants in urban settings?”_\n\n\n\nStep 5: Identify Patterns and Gaps\nWe’ll present the results across objectives, outcome, population and geography using intuitive, interactive visualizations:\n\nBubble maps (bubble size = number of studies or sample size)\n\nHeatmaps (showing concentration of evidence by topic or geography)\n\nGap maps (highlighting under-researched areas)\n\nThe evidence system will allow us to quickly highlight:\n\n✅ Areas with strong, consistent evidence\n\n⚠️ Topics with mixed or conflicting findings\n\n❌ Critical gaps where no evidence exists\n\nExample Insight:\n&gt; “Mentoring programs show consistent positive results for urban migrant youth, but there’s limited evidence for rural populations.”",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#deliverables",
    "href": "index.html#deliverables",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "Deliverables",
    "text": "Deliverables\nThe final deliverables will include:\n\nA Q&A dataset that can be used:\n\nAs a reference for content curation and evaluation\n\nAs a knowledge base for AI-enhanced project proposal systems\n\nAs a training dataset for fine-tuning small language models via Hugging Face\n\nA synthesis report identifying:\n\nResearch priorities\n\nHigh-risk areas for intervention\n\nRecommendations for future evaluations\n\nA searchable online visual evidence map for ongoing use by IOM teams that would allow to see evidence according to both SRF and GCM frameworks.",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "Developer Guide",
    "text": "Developer Guide\nThis module implements a Literate Programming approach, which means:\n\nDocumentation: Each handler serves as comprehensive documentation.\nCode Reference: The notebooks contain the actual implementation code.\nCommunication Tool: They facilitate discussions with data providers about discrepancies or inconsistencies.\n\nIf you are new to using nbdev, here are some useful pointers to get you started.\n\nInstall evaluation_knowledge in Development mode\n# make sure evaluation_knowledge package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to evaluation_knowledge\n$ nbdev_prepare",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/iom/evaluation_knowledge.git\nor from conda\n$ conda install -c iom evaluation_knowledge\nor from pypi\n$ pip install evaluation_knowledge\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "AI-Assisted Evaluation Evidence Mapping",
    "section": "How to use",
    "text": "How to use\n\nPrepare the documentation of your library\nRun the module",
    "crumbs": [
      "AI-Assisted Evaluation Evidence Mapping"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Workflow Description",
    "section": "",
    "text": "This notebook implements an evidence mapping system with: - Batch processing for scalability - Robust error handling and retries - Embedding caching - Hybrid search (vector + full-text) - Local LanceDB deployment\nwe can follow these steps:\n\nLoad the JSON file containing the URLs of the PDF reports.\nLoad the Excel file describing the IOM Results Framework.\nDownload and process the PDF reports to extract text.\nIntegrate the extracted text with the IOM Results Framework.\nGenerate embeddings and store them in LanceDB.",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#introduction",
    "href": "core.html#introduction",
    "title": "Workflow Description",
    "section": "",
    "text": "This notebook implements an evidence mapping system with: - Batch processing for scalability - Robust error handling and retries - Embedding caching - Hybrid search (vector + full-text) - Local LanceDB deployment\nwe can follow these steps:\n\nLoad the JSON file containing the URLs of the PDF reports.\nLoad the Excel file describing the IOM Results Framework.\nDownload and process the PDF reports to extract text.\nIntegrate the extracted text with the IOM Results Framework.\nGenerate embeddings and store them in LanceDB.",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#step-1-building-the-knowledge-base",
    "href": "core.html#step-1-building-the-knowledge-base",
    "title": "Workflow Description",
    "section": "Step 1: Building the Knowledge Base",
    "text": "Step 1: Building the Knowledge Base\nSo we have a collection of Evaluation documents. We have metadata for each Evaluation. For each evaluation, we have multiple documents (The evaluation report itslef, plus in some case: a summary brief, annexes, etc.)\nSee an example below\n\n[\n    {\n        \"Title\": \"Finale Internal Evluation: ENHANCING THE CAPACITY TO MAINSTREAM ENVIRONMENT AND CLIMATE CHANGE WITHIN WIDER FRAMEWORK OF MIGRATION MANAGEMENT IN WEST AND CENTRAL AFRICA\",\n        \"Year\": \"2022\",\n        \"Author\": \"Abderrahim El Moulat\",\n        \"Best Practices or Lessons Learnt\": \"Yes\",\n        \"Date of Publication\": \"2022-06-22 00:00:00\",\n        \"Donor\": \"IOM Development Fund\",\n        \"Evaluation Brief\": \"Yes\",\n        \"Evaluation Commissioner\": \"Donor, IOM\",\n        \"Evaluation Coverage\": \"Country\",\n        \"Evaluation Period From Date\": \"nan\",\n        \"Evaluation Period To Date\": \"NaT\",\n        \"Executive Summary\": \"Yes\",\n        \"External Version of the Report\": \"No\",\n        \"Languages\": \"English\",\n        \"Migration Thematic Areas\": \"Migration and climate change\",\n        \"Name of Project(s) Being Evaluated\": NaN,\n        \"Number of Pages Excluding annexes\": 20.0,\n        \"Other Documents Included\": NaN,\n        \"Project Code\": \"NC.0030\",\n        \"Countries Covered\": [\n            \"Senegal\"\n        ],\n        \"Regions Covered\": \"RO Dakar\",\n        \"Relevant Crosscutting Themes\": \"Gender\",\n        \"Report Published\": \"Yes\",\n        \"Terms of Reference\": \"No\",\n        \"Type of Evaluation Scope\": \"Programme/Project\",\n        \"Type of Evaluation Timing\": \"Ex-post (after the end of the project/programme)\",\n        \"Type of Evaluator\": \"Internal\",\n        \"Level of Evaluation\": \"Decentralized\",\n        \"Documents\": [\n            {\n                \"Document Subtype\": \"Evaluation brief\",\n                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Internal%20Evaluation_NC0030_JUNE_2022_FINAL_Abderrahim%20EL%20MOULAT_0.pdf\",\n                \"File description\": \"Evaluation Brief\"\n            },\n            {\n                \"Document Subtype\": \"Evaluation report\",\n                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/NC0030_Evaluation%20Brief_June%202022_Abderrahim%20EL%20MOULAT.pdf\",\n                \"File description\": \"Evaluation Report\"\n            }\n        ]\n    },\n    {\n        \"Title\": \"Local Authorities Network for Migration and Development\",\n        \"Year\": \"2022\",\n        \"Author\": \"Action Research for CO-development (ARCO)\",\n        \"Best Practices or Lessons Learnt\": \"No\",\n        \"Date of Publication\": \"2022-02-01 00:00:00\",\n        \"Donor\": \"Government of Italy\",\n        \"Evaluation Brief\": \"No\",\n        \"Evaluation Commissioner\": \"IOM\",\n        \"Evaluation Coverage\": \"Multi-country\",\n        \"Evaluation Period From Date\": \"2020-07-06 00:00:00\",\n        \"Evaluation Period To Date\": \"2021-07-31 00:00:00\",\n        \"Executive Summary\": \"Yes\",\n        \"External Version of the Report\": \"No\",\n        \"Languages\": \"English\",\n        \"Migration Thematic Areas\": \"Migration and Development - diaspora\",\n        \"Name of Project(s) Being Evaluated\": NaN,\n        \"Number of Pages Excluding annexes\": 37.0,\n        \"Other Documents Included\": NaN,\n        \"Project Code\": \"MD.0003\",\n        \"Countries Covered\": [\n            \"Albania\",\n            \"Italy\"\n        ],\n        \"Regions Covered\": \"RO Brussels\",\n        \"Relevant Crosscutting Themes\": \"Gender, Rights-based approach\",\n        \"Report Published\": \"No\",\n        \"Terms of Reference\": \"Yes\",\n        \"Type of Evaluation Scope\": \"Programme/Project\",\n        \"Type of Evaluation Timing\": \"Final (at the end of the project/programme)\",\n        \"Type of Evaluator\": \"External\",\n        \"Level of Evaluation\": \"Decentralized\",\n        \"Documents\": [\n            {\n                \"Document Subtype\": \"Evaluation report\",\n                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Evaluation%20Brief_ARCO_Shiraz%20JERBI.pdF\",\n                \"File description\": \"Evaluation Report \"\n            },\n            {\n                \"Document Subtype\": \"Evaluation brief\",\n                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Final%20evaluation%20report_ARCO_Shiraz%20JERBI_1.pdf\",\n                \"File description\": \"Evaluation Brief\"\n            },\n            {\n                \"Document Subtype\": \"Management response\",\n                \"File URL\": \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Management%20Response%20Matrix_ARCO_Shiraz%20JERBI.pdf\",\n                \"File description\": \"Management Response\"\n            }\n        ]\n    }\n]\n\nWorkflow Overview\n\nLoad external metadata for one evaluations as per json file.\nThen for each evaluation, download the file, convert it to PDF (in case it’s a word, excel or ppt), and load text for each document.\nThen implement a late chunking that can solve the lost context problem and insert in chunk table, enabling Hybrid Search capability so that search can be made based on both key words and similirarity.\nCreate additional metadata for both documents and for evaluation using an LLM call. the additional metadata shall help to define an asesssment of the “evidence strenght”. The metadata to be created are - evaluation type (formative, summative, impact), Methodology, study design, sample size, and data collection techniques\n\n\nInitialize LanceDB Vector Database\nThe database includes 23 tables:\n1. Evaluations Table Each row represents a unique evaluation with the following fields:\n\nevaluation_id (unique identifier)\ntitle\nauthor\npractice_or_lessons\ndonor\nis_brief\ncommissioner\ncoverage\ncountries\nfrom_date\nto_date\nhas_summary\nexternal_version\nlanguage\nthematic_area\nname_project\nproject_code\nevaluation_scope\nevaluation_timing\nevaluation_level\nevaluator_type\ntheme\ncross_cutting\n\nadditional variable will be generated through an LLM prompt on the entire evaluation content\n\nshort_title\nsummary\npopulation (PICO model)\nintervention (PICO model)\ncomparator (PICO model)\noutcome (PICO model)\nmethodology\nstudy_type\nstudy_design\nsample_size\ndata_collection_techniques\nevidence_strength\nlimitations\n\n2. Documents Table\nEach row represents a PDF file linked to an evaluation:\n\ndocument_id: Primary key ID of the original PDF\nevaluation_id: foreign key to link to the evaluation\ndocument_subtype: from the original metadata\ndocument_url: from the original metadata\ndocument_name: from the original metadata\ndocument_tite: document type as reviewed by the LLM\ndocument_type_infer: document type as reviewed by the LLM\ndocument_processed: boolean to confirm it is done\n\n3. Chunk Table * chunk_id: Primary key\n* evaluation_id: foreign key to link to the evaluation * document_id: ID of the original file * document_page: for proper referencing of any further information retrieval * chunk_index: order of the chunk in the document * text: the chunked content * embedding (for hybrid search)\nLet’s start by loading the library from json…\n\nsource\n\n\nload_evaluations\n\n load_evaluations (json_path:str)\n\n*Load evaluation data from a JSON file\nArgs: json_path: Path to the JSON file containing evaluation data\nReturns: List of evaluation dictionaries*\nLoad a small subset for testing..\n\n# Load your   metadata\n#evaluation_data =  load_evaluations(\"reference/Evaluation_repository_test.json\")\nevaluation_data =  load_evaluations(\"reference/Evaluation_repository.json\")\nprint(f\"Attribute name is: {evaluation_data}\")\nprint(type(evaluation_data))\n\nId Generation\n\nsource\n\n\ngenerate_id\n\n generate_id (text:str)\n\nGenerate a deterministic ID from text\n\neval_id = generate_id( \"aaa\")\nprint({eval_id})\n\n\nsource\n\n\nforce_delete_directory\n\n force_delete_directory (path, max_retries=3, delay=1)\n\nRobust directory deletion with retries and delay\n\nforce_delete_directory(LANCE_DB_PATH)\n\nWe start prefilling our vector database with the metadata\n\nsource\n\n\ninitialise_knowledge_base\n\n initialise_knowledge_base (db, evaluation:Dict)\n\nStore full documents without chunking (late chunking approach)\n\nsource\n\n\nsafe_get\n\n safe_get (d, key, default=None)\n\nSafely get value from dict, handle NaN and missing keys\n\nLANCE_DB_PATH = \"./lancedb\"\ndb = connect(LANCE_DB_PATH)\nfor evaluation in evaluation_data:  # Assuming evaluation_data is a list\n    initialise_knowledge_base(db, evaluation)\n\nLet’s check each evaluation is in the DB -\n\neval_table = db.open_table(\"evaluations\")\n#  Convert to Pandas DataFrame (recommended for display)\ndf = eval_table.to_pandas()\nprint(df)\n\nand the corresponding documents…\n\nLANCE_DB_PATH = \"./lancedb\"\nfrom lancedb import connect\ndb = connect(LANCE_DB_PATH)\ndoc_table = db.open_table(\"documents\")\n#  Convert to Pandas DataFrame (recommended for display)\ndf = doc_table.to_pandas()\nprint(df)\n# this table includes document_id, url, and evaluation_id\n\n\n\nDownload and prepare all the files\nNow we build a smart function to download the files from URL: - this function takes an argument the doc_table from the vector DB (doc_table = db.open_table(\"documents\")). this table includes document_id, url, and evaluation_id - then for each document, and in parallelised way, it loads the url and extract the file_name from the url within the table - it builds a local file_path with PDF_Library/evaluation_id/file_name (where PDF_Library is an environment variable) - it checks if the ‘file_name’ is already present and then gracefully exit - if not, it downloads the file_name - this done with with some provision to avoid requesting IP being banned - and ensure some retry until the file is downloaded - if the file_name extension is not pdf, it identify the file extension then it converts it to pdf\n\nsource\n\n\ndownload_documents\n\n download_documents (doc_table)\n\nHere is the file conversion functions that assumes that libre-office is installed locally.\n# Debian/Ubuntu\nsudo apt install libreoffice\n\n# Mac (Homebrew)\nbrew install --cask libreoffice\n\nsource\n\n\nconvert_file_to_pdf\n\n convert_file_to_pdf (input_path, output_path)\n\nConverts Word, Excel, or PowerPoint file to PDF using LibreOffice in headless mode. Works on Windows, macOS, and Linux.\n\nsource\n\n\nfind_libreoffice_exec\n\n find_libreoffice_exec ()\n\nFinds the appropriate LibreOffice command based on OS. Returns path to LibreOffice CLI tool or raises an error.\nTesting this…\n\ndoc_table = db.open_table(\"documents\")\nos.environ[\"PDF_Library\"] = \"Evaluation_Library\"\ndownload_documents(doc_table)\n\n\n\nNow load file content in the vector DB, chunk and embedd\nBuilding a function that - this function takes an argument the doc_table from the vector DB (doc_table = db.open_table(\"documents\")). this table includes document_id, url, and evaluation_id, processed - then for each document, and in parallelised way, it loads the url and extract the file_name from the url within the table - it assume a local file_path with PDF_Library/evaluation_id/file_name (where PDF_Library is an environment variable - file_name is extracted from the url - and the file_name extension is sanitised to include systematically ‘.pdf’ ) - It will extract the text from the PDF using PyMuPDF with error handling – it will implement the It will then fill in the chunk table in lancedb, implementing a late chunking approach to avoid duplicate embedding computation, ensure context-aware chunk boundaries and precise span tracking . - the lancedb chunk table schema should be - chunk_id: str - document_id: str - evaluation_id: str - metadata: dict[str, str] - content: str = embedding_fn.SourceField() - vector: Vector(embedding_fn.ndims()) = embedding_fn.VectorField() - Once processed the processed variable in doc_table is set to true\nTest the embeddings through lanchain….\n\ntest_embedding = embedding_model.embed_query(\"Hello world\")\nprint(f\"Embedding vector length: {len(test_embedding)}\")\nembedding_dim = len(test_embedding)\n# LanceDB-compatible wrapper\nclass LangchainEmbeddingWrapper:\n    def __init__(self, langchain_embedder):\n        self._embedder = langchain_embedder\n\n    def __call__(self, texts):\n        return self._embedder.embed_documents(texts)\n        \n    def ndims(self):\n        return self._dim\n\n# Wrap and use\nembedding_fn = LangchainEmbeddingWrapper(embedding_model)\n#print(\"Embedding dimension:\", embedding_fn.ndims())\n\nvec = embedding_fn([\"Hello world\"])\nprint(f\"Vector through lancedb dim: {len(vec[0])}\")\nprint(embedding_fn([\"Hello world\"])[0])\n\n\nprint(dir(embedding_fn))\nhelp(embedding_fn)\n\nSo first we create the chunk table in lancedb\n\nfrom pydantic import BaseModel\nfrom lancedb.pydantic import Vector\nimport pyarrow as pa \npa_schema = pa.schema([\n    pa.field(\"chunk_id\", pa.string()),\n    pa.field(\"document_id\", pa.string()),\n    pa.field(\"evaluation_id\", pa.string()),\n    pa.field(\"metadata\", pa.string()),  # storing metadata dict as JSON string for simplicity\n    pa.field(\"content\", pa.string()),\n    pa.field(\"vector\", pa.list_(pa.float32(), embedding_dim)),  # vector as list of floats\n])\n\nfrom lancedb import connect\ndb = connect(LANCE_DB_PATH)\nchunk_table = db.create_table(\"chunks\", schema=pa_schema)\n\nand then the function creating embeddings chunck for each document\n\nsource\n\n\nprocess_documents_to_chunks\n\n process_documents_to_chunks (doc_table, chunk_table)\n\nNow let’s run this!\n\nLANCE_DB_PATH = \"./lancedb\"\nfrom lancedb import connect\ndb = connect(LANCE_DB_PATH)\ndoc_table = db.open_table(\"documents\")\nchunk_table = db.open_table(\"chunks\")\nprocess_documents_to_chunks(doc_table, chunk_table)\n\nChecking the status of the chunking process\n\nsource\n\n\ncheck_chunk_status\n\n check_chunk_status (doc_table, chunk_table)\n\n\nLANCE_DB_PATH = \"./lancedb\"\nfrom lancedb import connect\ndb = connect(LANCE_DB_PATH)\ndoc_table = db.open_table(\"documents\")\nchunk_table = db.open_table(\"chunks\")\nmissing_docs_df = check_chunk_status(doc_table, chunk_table)\nprint(missing_docs_df)\n# this table includes document_id, url, and evaluation_id\n\n\n\nGenerating AI-Enhanced metadata\nLast, we run a function to generate metadata…\nThe function will load the “evaluations” table within the db –connect(LANCE_DB_PATH) – then loop around each evaluation_id within the “chunks” table to retrive the context - and perform an LLM call to then generate as an output additional metadata Then it will update the evaluations table with the output for each evaluation - At the end it will save a json file with a dump of the evaluations table\n\nsource\n\n\nget_context_for_eval\n\n get_context_for_eval (eval_row, query, chunk_table)\n\n\nsource\n\n\ncall_llm_with_retries\n\n call_llm_with_retries (prompt, max_retries=4, delay=2)\n\n\nsource\n\n\nsafe_join\n\n safe_join (items, sep=', ')\n\n\nsource\n\n\nclean_json\n\n clean_json (obj)\n\nRecursively clean an object to make it JSON serializable, handling None values.\nWe will use the PICO structured framework as an approach to represent the causal knowledge found in the Evaluation (cf. EconBERTa: Towards Robust Extraction of Named Entities in Economics). This scheme helps in systematically organizing and analyzing the effectiveness of interventions by comparing outcomes between groups:\n1. Population (P): The group of individuals or units (e.g., households, schools, firms) affected by the intervention. The target population shall be clearly defined (e.g., smallholder farmers, primary school students, unemployed youth) and it shall Include eligibility criteria (e.g., age, socioeconomic status, geographic location).\n2. Intervention (I): The program, policy, or treatment whose effect is being evaluated. Describes the active component being tested (e.g., cash transfers, training workshops, new teaching methods). Should specify dosage, duration, and delivery mechanism.\n3. Comparators (C): The counterfactual scenario—what would have happened without the intervention. Ideally involves a control group (if the study approach is randomized or quasi-experimental) that does not receive the intervention. Alternatively refers to “Business-as-usual” groups, placebo interventions, or different treatment arms.\n4. Outcome (O):The measurable effects or endpoints used to assess the intervention’s impact. Includes primary outcomes (main indicators of interest, e.g., school enrollment rates, income levels) and secondary outcomes (e.g., health, empowerment). Should be specific, measurable, and time-bound (e.g., “child literacy scores after 12 months”).\nUsing such approach, we can ensure Clarity (the research question is well-defined and testable), Causal Inference (isolate the effect of the intervention by comparing treated and untreated groups), Replicability (to potentially extrapolate the findings) and Relevance (linking outcomes to real-world decision-making).\n\nsource\n\n\ngenerate_metadata_for_evaluation_metadata_descriptive\n\n generate_metadata_for_evaluation_metadata_descriptive (eval_row,\n                                                        query_descriptive,\n                                                        chunk_table)\n\nProcess one evaluation row and return updated row with metadata.\n\nsource\n\n\ngenerate_metadata_for_evaluation_metadata_methodo\n\n generate_metadata_for_evaluation_metadata_methodo (eval_row,\n                                                    query_methodo,\n                                                    chunk_table)\n\nProcess one evaluation row and return updated row with metadata.\n\nsource\n\n\ngenerate_metadata_for_evaluation_metadata_evidence\n\n generate_metadata_for_evaluation_metadata_evidence (eval_row, query,\n                                                     chunk_table)\n\nProcess one evaluation row and return updated row with metadata.\n\nsource\n\n\ngenerate_evaluation_metadata\n\n generate_evaluation_metadata (eval_table, chunk_table, batch_size=10,\n                               output_file='all_evaluations_metadata.json'\n                               )\n\nMain function to generate metadata in batches and update the table, with incremental saving.\nNow let’s run it!\n\n# Initialize DB\nLANCE_DB_PATH = \"./lancedb\"\ndb = connect(LANCE_DB_PATH)\neval_table = db.open_table(\"evaluations\")\nchunk_table = db.open_table(\"chunks\")\nenriched_data= generate_evaluation_metadata(eval_table, chunk_table, output_file=\"all_evaluations_metadata2.json\")",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#step-2-structured-information-extraction",
    "href": "core.html#step-2-structured-information-extraction",
    "title": "Workflow Description",
    "section": "Step 2: Structured Information Extraction",
    "text": "Step 2: Structured Information Extraction\n\nStandard Questions\n# Define the list of experts on impact - outcome - organisation\nq_experts = [\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Attaining favorable protection environments---: i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Realizing rights in safe environments---: i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Empowering communities and achieving gender equality--- : i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the Strategic Impact: ---Securing durable solutions--- : i.e., finding or recommendations that require a change in existing policy and regulations. [/INST]\",\n\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: ---Access to territory registration and documentation ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Status determination ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Protection policy and law---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Gender-based violence ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Child protection ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Safety and access to justice ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Community engagement and women's empowerment ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Well-being and basic needs ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Sustainable housing and settlements ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Healthy lives---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Education ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Clean water sanitation and hygiene ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Self-reliance, Economic inclusion, and livelihoods ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Voluntary repatriation and sustainable reintegration ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Resettlement and complementary pathways---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on the specific Operational Outcome: --- Local integration and other local solutions ---, i.e. finding or recommendations that require a change that needs to be implemented in the field as an adaptation or change of current activities. [/INST]\", \n\n\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Systems and processes, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\",\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Operational support and supply chain, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to People and culture, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to External engagement and resource mobilization, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" ,\n   \"&lt;s&gt; [INST] Instructions: Act as a public program evaluation expert working for UNHCR. Your specific area of expertise and focus is strictly on Organizational Enablers related to Leadership and governance, i.e. elements that require potential changes in either management practices, technical approach, business processes, staffing allocation or capacity building. [/INST]\" \n]\n\n# Predefined knowledge extraction questions\nq_questions = [\n    \" List, as bullet points, all findings and evidences in relation to your specific area of expertise and focus. \",\n    \" Explain, in relation to your specific area of expertise and focus, what are the root causes for the situation. \" ,\n    \" Explain, in relation to your specific area of expertise and focus, what are the main risks and difficulties here described. \",\n    \" Explain, in relation to your specific area of expertise and focus, what what can be learnt. \",\n    \" List, as bullet points, all recommendations made in relation to your specific area of expertise and focus. \"#,\n    # \"Indicate if mentionnend what resource will be required to implement the recommendations made in relation to your specific area of expertise and focus. \",\n    # \"List, as bullet points, all recommendations made in relation to your specific area of expertise and focus that relates to topics  or activities recommended to be discontinued. \",\n    # \"List, as bullet points, all recommendations made in relation to your specific area of expertise and focus that relates to topics or activities recommended to be scaled up. \" \n    # Add more questions here...\n]\n\n## Additional instructions!\nq_instr = \"\"\"\n&lt;/s&gt;\n[INST]  \nKeep your answer grounded in the facts of the contexts. \nIf the contexts do not contain the facts to answer the QUESTION, return {NONE} \nBe concise in the response and  when relevant include precise citations from the contexts. \n[/INST] \n\"\"\"\n\n\nQ&A Extraction\nqa_questions = [\n    \"What was the intervention type?\",\n    \"What outcomes were observed?\",\n    \"What population was targeted?\",\n    \"What geographic area was covered?\",\n    \"How strong is the evidence?\",\n]\n\ndef generate_qas(text):\n    prompt = f\"\"\"Extract answers to the following questions from the evaluation:\n    {json.dumps(qa_questions)}\n    \n    Text: {text[:3000]}\n    \"\"\"\n    completion = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.2\n    )\n    return completion.choices[0].message.content\n\ndf_docs[\"qa\"] = df_docs[\"text\"].apply(generate_qas)\n\n\nHybrid Search in LanceDB\n# Sample hybrid search query\nquery = \"What works best to improve health outcomes for displaced persons?\"\n\nquery_embedding = get_text_embedding(query)\nresults = table.search(query_embedding).limit(5).to_list()\n\nfor result in results:\n    print(result['metadata'])\n    print(result['text'][:500])\ndef query_evidence(question: str, table: lancedb.db.LanceTable) -&gt; Dict:\n    \"\"\"Enhanced query with hybrid search and evidence grading\"\"\"\n    try:\n        # Hybrid search\n        results = hybrid_search(table, question, limit=7)\n        \n        if results.empty:\n            return {\"answer\": \"No relevant evidence found.\", \"sources\": []}\n        \n        context = \"\\n\\n\".join([\n            f\"Document {i+1} (Relevance: {row.get('combined_score', 0):.2f}):\\n{row['text']}\\n\"\n            for i, row in results.iterrows()\n        ])\n        \n        # Evidence-based answer generation\n        prompt = f\"\"\"\n        You are an evidence specialist answering questions about IOM programs.\n        Use ONLY the provided context from evaluation reports.\n        For each claim in your answer, cite the document number it came from.\n        \n        Question: {question}\n        \n        Context:\n        {context}\n        \n        Provide:\n        1. A direct answer to the question\n        2. Strength of evidence (High/Medium/Low)\n        3. Any limitations or caveats\n        4. List of sources with relevance scores\n        \"\"\"\n        \n        response = openai.ChatCompletion.create(\n            engine=config.chat_model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=config.temperature,\n            max_tokens=config.max_tokens\n        )\n        \n        answer = response.choices[0].message.content\n        sources = [\n            {\"url\": url, \"score\": score}\n            for url, score in zip(results['url'], results.get('combined_score', 0))\n        ]\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"sources\": sources,\n            \"search_scores\": results[['_distance', '_score', 'combined_score']].to_dict()\n        }\n    \n    except Exception as e:\n        print(f\"Query error: {str(e)}\")\n        return {\"error\": str(e)}\n\ndef extract_structured_info(table, iom_framework):\n    \"\"\"Extract structured information from reports using the IOM Results Framework\"\"\"\n    # Generate questions based on the IOM framework\n    questions = generate_questions_from_framework(iom_framework)\n    \n    # Store extracted information\n    extracted_data = []\n    \n    # Process each question\n    for question in questions:\n        print(f\"Processing question: {question}\")\n        \n        # Search for relevant chunks\n        results = table.search(generate_embeddings([question])[0]).limit(10).to_pandas()\n        \n        # Combine relevant chunks as context\n        context = \"\\n\\n\".join(results[\"text\"].tolist())\n        \n        # Use Azure OpenAI to extract structured answer\n        prompt = f\"\"\"\n        Based on the following evaluation report excerpts, answer the question with structured information.\n        Provide your answer in JSON format with the following structure:\n        {{\n            \"question\": \"the question being asked\",\n            \"answer\": \"the concise answer\",\n            \"intervention_type\": \"type of intervention mentioned\",\n            \"population\": \"target population mentioned\",\n            \"outcome\": \"outcome measured\",\n            \"geography\": \"geographic location if mentioned\",\n            \"evidence_strength\": \"strength of evidence (high/medium/low)\"\n        }}\n\n        Question: {question}\n        Context: {context}\n        \"\"\"\n        \n        response = openai.ChatCompletion.create(\n            engine=config[\"azure_openai_chat_deployment\"],\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=config[\"temperature\"],\n            max_tokens=config[\"max_tokens\"]\n        )\n        \n        try:\n            answer = json.loads(response.choices[0].message.content)\n            answer[\"source_urls\"] = results[\"url\"].unique().tolist()\n            extracted_data.append(answer)\n        except json.JSONDecodeError:\n            print(f\"Failed to parse answer for question: {question}\")\n    \n    return pd.DataFrame(extracted_data)\n\ndef generate_questions_from_framework(framework_df):\n    \"\"\"Generate questions based on the IOM Results Framework\"\"\"\n    questions = []\n    \n    # Example questions based on common evaluation themes\n    for _, row in framework_df.iterrows():\n        questions.extend([\n            f\"What interventions has IOM implemented to achieve {row['Objective']}?\",\n            f\"What evidence exists for the effectiveness of interventions targeting {row['Outcome']}?\",\n            f\"What populations have been targeted by interventions aiming for {row['Indicator']}?\",\n            f\"What geographic areas have seen interventions related to {row['Objective']}?\",\n            f\"What methodologies have been used to evaluate interventions for {row['Outcome']}?\"\n        ])\n    \n    # Add some general evaluation questions\n    questions.extend([\n        \"What are the most effective interventions for migrant livelihood improvement?\",\n        \"What evidence exists for cash-based interventions in migration contexts?\",\n        \"What are common challenges in implementing migration programs?\",\n        \"What evaluation methodologies are most commonly used in IOM evaluations?\",\n        \"What gaps exist in the evidence base for migration interventions?\"\n    ])\n    \n    return list(set(questions))  # Remove duplicates\n\ndef hybrid_search(table: lancedb.db.LanceTable, query: str, limit: int = 10) -&gt; pd.DataFrame:\n    \"\"\"Perform hybrid (vector + full-text) search\"\"\"\n    # Generate query embedding\n    query_embedding = generate_embeddings_batch([query])[0]\n    \n    # Perform hybrid search\n    results = table.search(query_embedding, query_string=query)\\\n                 .limit(limit)\\\n                 .to_pandas()\n    \n    # Score normalization (simple example)\n    if not results.empty:\n        max_vector_score = results[\"_distance\"].max()\n        max_fts_score = results[\"_score\"].max()\n        \n        if max_vector_score &gt; 0 and max_fts_score &gt; 0:\n            results[\"combined_score\"] = (\n                0.7 * (results[\"_distance\"] / max_vector_score) +\n                0.3 * (results[\"_score\"] / max_fts_score)\n            )\n            results = results.sort_values(\"combined_score\", ascending=False)\n    \n    return results\n\ndef extract_structured_info(table: lancedb.db.LanceTable, iom_framework: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Enhanced information extraction with hybrid search\"\"\"\n    questions = generate_questions_from_framework(iom_framework)\n    extracted_data = []\n    \n    for question in questions:\n        try:\n            # Hybrid search for relevant chunks\n            results = hybrid_search(table, question, limit=15)\n            \n            if results.empty:\n                continue\n                \n            context = \"\\n\\n\".join(results[\"text\"].tolist())\n            sources = results[\"url\"].unique().tolist()\n            \n            # Structured extraction prompt\n            prompt = f\"\"\"\n            Extract structured information from this evaluation report context to answer the question.\n            Return ONLY valid JSON with this structure:\n            {{\n                \"question\": \"the question\",\n                \"answer\": \"concise answer\",\n                \"intervention_type\": [\"type1\", \"type2\"],\n                \"population\": [\"group1\", \"group2\"],\n                \"outcome\": [\"outcome1\", \"outcome2\"],\n                \"geography\": [\"location1\", \"location2\"],\n                \"evidence_strength\": \"high/medium/low\",\n                \"source_urls\": [\"url1\", \"url2\"]\n            }}\n            \n            Question: {question}\n            Context: {context}\n            \"\"\"\n            \n            response = openai.ChatCompletion.create(\n                engine=config.chat_model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=config.temperature,\n                max_tokens=config.max_tokens,\n                response_format={ \"type\": \"json_object\" }\n            )\n            \n            answer = json.loads(response.choices[0].message.content)\n            answer[\"source_urls\"] = sources\n            extracted_data.append(answer)\n            \n        except Exception as e:\n            print(f\"Error processing question '{question}': {str(e)}\")\n            continue\n    \n    return pd.DataFrame(extracted_data)",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#step-3-organize-the-evidence",
    "href": "core.html#step-3-organize-the-evidence",
    "title": "Introduction",
    "section": "Step 3: Organize the Evidence",
    "text": "Step 3: Organize the Evidence\ndef extract_information(text):\n    # Use Azure OpenAI to extract information\n    response = openai.Completion.create(\n        engine=\"your-completion-engine\",\n        prompt=f\"Extract information from: {text}\",\n        max_tokens=150\n    )\n    return response.choices[0].text.strip()\n\ndf['structured_info'] = df['text'].apply(extract_information)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "core.html#step-4-generate-actionable-and-generalizable-insights",
    "href": "core.html#step-4-generate-actionable-and-generalizable-insights",
    "title": "Workflow Description",
    "section": "Step 4: Generate Actionable and Generalizable Insights",
    "text": "Step 4: Generate Actionable and Generalizable Insights\nOne key challenge is How to generalize the findings from an evaluation from one place to another one? The Generalizability Framework provides some insights on how to do that.\nTo implement this we will generate insights using AI-enabled Q&A on all previous Q&A:\ndef generate_insights(df):\n    # Add your insight generation logic here\n    return df\n\ndf = generate_insights(df)",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#step-5-identify-patterns-and-gaps",
    "href": "core.html#step-5-identify-patterns-and-gaps",
    "title": "Workflow Description",
    "section": "Step 5: Identify Patterns and Gaps",
    "text": "Step 5: Identify Patterns and Gaps\nIdentify patterns and gaps in the data:\ndef identify_patterns(df):\n    # Add your pattern identification logic here\n    return df\n\ndf = identify_patterns(df)\ndef generate_deliverables(df):\n    # Generate Q&A dataset\n    qa_dataset = df[['question', 'answer']]\n\n    # Generate synthesis report\n    synthesis_report = df.describe()\n\n    # Generate visual evidence map\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='outcome', y='population', size='sample_size')\n    plt.title('Visual Evidence Map')\n    plt.show()\n\n    return qa_dataset, synthesis_report\n\nqa_dataset, synthesis_report = generate_deliverables(df)\nVisualize Patterns & Gaps\n# Convert QA to structured fields (intervention, outcome, population, etc.)\nqa_df = pd.json_normalize(df_docs[\"qa\"].apply(json.loads))\n\n# Bubble Map Example\nfig = px.scatter(qa_df, x=\"geography\", y=\"outcome\",\n                 size=\"sample_size\", color=\"intervention\",\n                 hover_name=\"file_name\",\n                 title=\"Evidence Bubble Map\")\nfig.show()\n\n# Heatmap Example\nheatmap_df = pd.crosstab(qa_df[\"intervention\"], qa_df[\"outcome\"])\nsns.heatmap(heatmap_df, annot=True, cmap=\"coolwarm\")\ndef create_interactive_visualizations(extracted_data: pd.DataFrame):\n    \"\"\"Enhanced visualization functions\"\"\"\n    # Prepare data\n    df = extracted_data.explode(\"source_urls\")\n    \n    # Evidence Strength Distribution\n    strength_dist = df['evidence_strength'].value_counts().reset_index()\n    fig1 = px.bar(\n        strength_dist,\n        x='evidence_strength',\n        y='count',\n        title='Distribution of Evidence Strength'\n    )\n    \n    # Interventions by Geography\n    fig2 = px.treemap(\n        df,\n        path=['geography', 'intervention_type'],\n        title='Interventions by Geographic Region'\n    )\n    \n    # Evidence Timeline (if dates available)\n    if 'date' in df.columns:\n        fig3 = px.line(\n            df.groupby('date').size().reset_index(name='count'),\n            x='date',\n            y='count',\n            title='Evidence Publication Timeline'\n        )\n    else:\n        fig3 = None\n    \n    return fig1, fig2, fig3\n\ndef visualize_evidence_map(extracted_data):\n    \"\"\"Create interactive visualizations of the evidence map\"\"\"\n    \n    # Prepare data for visualization\n    df = extracted_data.explode(\"source_urls\")\n    \n    # Bubble map: Interventions by outcome and evidence strength\n    fig1 = px.scatter(\n        df, \n        x=\"outcome\", \n        y=\"intervention_type\", \n        size=\"evidence_strength\",  # This would need to be mapped to numeric values\n        color=\"population\",\n        hover_name=\"answer\",\n        title=\"Evidence Map: Interventions by Outcome and Population\"\n    )\n    fig1.update_layout(height=800)\n    \n    # Heatmap: Evidence concentration by intervention and outcome\n    heatmap_data = df.groupby(['intervention_type', 'outcome']).size().unstack().fillna(0)\n    fig2 = px.imshow(\n        heatmap_data,\n        labels=dict(x=\"Outcome\", y=\"Intervention Type\", color=\"Number of Studies\"),\n        title=\"Evidence Concentration Heatmap\"\n    )\n    \n    # Gap map: Missing evidence\n    all_interventions = df['intervention_type'].unique()\n    all_outcomes = df['outcome'].unique()\n    complete_grid = pd.MultiIndex.from_product([all_interventions, all_outcomes], names=['intervention_type', 'outcome'])\n    gap_data = df.groupby(['intervention_type', 'outcome']).size().reindex(complete_grid, fill_value=0).reset_index()\n    gap_data['has_evidence'] = gap_data[0] &gt; 0\n    \n    fig3 = px.scatter(\n        gap_data,\n        x=\"outcome\",\n        y=\"intervention_type\",\n        color=\"has_evidence\",\n        title=\"Evidence Gap Map (Red = Missing Evidence)\"\n    )\n    \n    return fig1, fig2, fig3\n\ndef generate_synthesis_report(extracted_data):\n    \"\"\"Generate a narrative synthesis report of findings\"\"\"\n    prompt = f\"\"\"\n    You are an evaluation specialist analyzing evidence from IOM evaluation reports.\n    Below is structured data extracted from multiple evaluation reports:\n    \n    {extracted_data.to_json()}\n    \n    Write a comprehensive synthesis report that:\n    1. Summarizes key findings across interventions\n    2. Identifies areas with strong evidence\n    3. Highlights evidence gaps\n    4. Provides recommendations for future evaluations\n    5. Suggests high-priority research areas\n    \n    Structure your report with clear sections and bullet points for readability.\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        engine=config[\"azure_openai_chat_deployment\"],\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.5,  # Slightly more creative for synthesis\n        max_tokens=3000\n    )\n    \n    return response.choices[0].message.content\n\nSave the deliverables to files:\nqa_dataset.to_csv('qa_dataset.csv', index=False)\nsynthesis_report.to_csv('synthesis_report.csv')",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#conclusions---and-potential-extension",
    "href": "core.html#conclusions---and-potential-extension",
    "title": "Workflow Description",
    "section": "Conclusions - and potential extension…",
    "text": "Conclusions - and potential extension…\n\nWeb interface (Streamlit, Gradio, etc.)\nPeriodic syncing with new evaluations via web scraping\nIntegration with Hugging Face for fine-tuning a summarization or Q&A model",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Workflow Description",
    "section": "Setup",
    "text": "Setup\n\nCreate a Virtual Environment\nTo ensure a clean and isolated environment for this project, we will create a virtual environment using Python’s venv module. This will help manage dependencies and avoid conflicts with other projects.\n#| eval: false\npython -m venv .venv\nThen, activate the virtual environment:\n#| eval: false\n.\\.venv\\Scripts\\activate\nThen, configure visual Studio Code to use the virtual environment: Open the Command Palette using the shortcut Ctrl+Shift+P and type Jupyter: Select Interpreter and select the interpreter that corresponds to your newly created virtual environment: ('venv': venv).\n\n\nRequired Python Modules\nOnce this environment selected as a kernel to run the notebook, we can install the required python modules the rest of the process:\n%pip install openai  lancedb pyarrow pandas numpy matplotlib seaborn plotly pymupdf requests tqdm tenacity ipython dotenv langchain langchain-community langchain_openai  ipywidgets openpyxl  filetype\nthen Restart the jupyter kernel for this notebook\n#| eval: false\n%reset -f\n\n\nInitialise LLM API\n\n\nLoad PDF library and Strategic Results Framework\nThe library\n\nsource\n\n\nload_evaluations\n\n load_evaluations (file_path, json_path)\n\n\nlibrary =load_evaluations(\"reference/Evaluation_repository.xlsx\",\"reference/Evaluation_repository.json\" )\n\nNow the framework\n\nsource\n\n\nload_iom_framework\n\n load_iom_framework (excel_path:str)\n\nLoad and validate IOM framework\n\nframework= load_iom_framework(\"reference/Strategic_Result_Framework.xlsx\")",
    "crumbs": [
      "Workflow Description"
    ]
  },
  {
    "objectID": "core.html#step-3-cross-evaluation-analysis",
    "href": "core.html#step-3-cross-evaluation-analysis",
    "title": "Workflow Description",
    "section": "Step 3: Cross-Evaluation Analysis",
    "text": "Step 3: Cross-Evaluation Analysis",
    "crumbs": [
      "Workflow Description"
    ]
  }
]